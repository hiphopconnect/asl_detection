{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutze Device: cpu\n",
      "Modell erfolgreich geladen!\n",
      "Starte Echtzeit-Erkennung... (Drücke 'q' zum Beenden)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741178096.818304    1301 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741178096.866721    1301 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Modellklasse definieren (identisch zum Training)\n",
    "class HandSignNet(nn.Module):\n",
    "    def __init__(self, num_classes=24):\n",
    "        super(HandSignNet, self).__init__()\n",
    "        \n",
    "        # Feature Extraction Blocks\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(63, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Landmark-Extraktion für 63 Features (exakt wie im Training)\n",
    "def extract_landmarks(results):\n",
    "    \"\"\"\n",
    "    Extrahiert Keypoints der signierenden Hand (rechte Hand aus Sicht des Betrachters)\n",
    "    Exakt gleiche Methode wie im Trainingscode\n",
    "    \"\"\"\n",
    "    # Initialisiere Array für die signierende Hand (21 Keypoints mit x, y, z)\n",
    "    hand_keypoints = np.zeros(21 * 3)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Wenn mehrere Hände erkannt wurden, finde die richtige Hand\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Die Hand-Klassifikation ist aus Sicht der Kamera\n",
    "            handedness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            if handedness == \"Right\":  # Wir suchen die rechte Hand aus Sicht der Kamera\n",
    "                hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "                break\n",
    "        # Falls keine rechte Hand gefunden wurde, nimm die erste erkannte Hand\n",
    "        if np.all(hand_keypoints == 0) and results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    return hand_keypoints\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Bild wird unverändert zurückgegeben (keine Transformationen)\n",
    "    \"\"\"\n",
    "    return image\n",
    "\n",
    "class ASLPredictor:\n",
    "    def __init__(self, model_path='/workspaces/asl_detection/machine_learning/models/asl_now/best_model.pth'):\n",
    "        # MediaPipe Hands initialisieren (wie beim Training)\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.2,\n",
    "            min_tracking_confidence=0.2)\n",
    "\n",
    "        # Modell laden\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Nutze Device: {self.device}\")\n",
    "        \n",
    "        self.model = HandSignNet().to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Buchstaben-Mapping (alle Buchstaben außer j und z)\n",
    "        self.letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
    "        print(\"Modell erfolgreich geladen!\")\n",
    "        \n",
    "        # Puffer für stabilere Vorhersagen\n",
    "        self.prediction_buffer = []\n",
    "        self.buffer_size = 5\n",
    "        self.last_prediction = \"\"\n",
    "\n",
    "    def predict_frame(self, frame):\n",
    "        \"\"\"Verarbeitet ein Frame und gibt die Vorhersage zurück\"\"\"\n",
    "        # Bild vorverarbeiten (jetzt ohne Transformationen)\n",
    "        frame = preprocess_image(frame)\n",
    "        \n",
    "        # Konvertiere zu RGB für MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(frame_rgb)\n",
    "        \n",
    "        # Zeichne Handpunkte\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    hand_landmarks, \n",
    "                    self.mp_hands.HAND_CONNECTIONS,\n",
    "                    self.mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    self.mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        # Mache Vorhersage wenn Hand erkannt wurde (exakt wie im Training)\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Features extrahieren\n",
    "            landmarks = extract_landmarks(results)\n",
    "            \n",
    "            # Modellvorhersage\n",
    "            with torch.no_grad():\n",
    "                landmarks_tensor = torch.FloatTensor(landmarks).unsqueeze(0).to(self.device)\n",
    "                outputs = self.model(landmarks_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, prediction = torch.max(probabilities, dim=1)\n",
    "                \n",
    "                # Hole Buchstaben und Konfidenz\n",
    "                predicted_letter = self.letters[prediction.item()]\n",
    "                confidence_value = confidence.item()\n",
    "                \n",
    "                # Vorhersage zum Puffer hinzufügen\n",
    "                self.prediction_buffer.append(predicted_letter)\n",
    "                \n",
    "                # Puffer-Größe begrenzen\n",
    "                if len(self.prediction_buffer) > self.buffer_size:\n",
    "                    self.prediction_buffer.pop(0)\n",
    "                \n",
    "                # Häufigste Vorhersage auswählen\n",
    "                if self.prediction_buffer:\n",
    "                    most_common = Counter(self.prediction_buffer).most_common(1)\n",
    "                    self.last_prediction = most_common[0][0]\n",
    "                    frequency = most_common[0][1] / len(self.prediction_buffer)\n",
    "                    \n",
    "                    # Zeige Vorhersage an\n",
    "                    if frequency > 0.6:  # Nur anzeigen wenn mehr als 60% der Vorhersagen übereinstimmen\n",
    "                        cv2.rectangle(frame, (0, 0), (200, 100), (245, 117, 16), -1)\n",
    "                        cv2.putText(frame, self.last_prediction.upper(), \n",
    "                                  (60, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame, f\"Konf: {confidence_value:.2f}\", \n",
    "                                  (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    # Initialisiere Predictor\n",
    "    predictor = ASLPredictor()\n",
    "    \n",
    "    # Öffne Webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    print(\"Starte Echtzeit-Erkennung... (Drücke 'q' zum Beenden)\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Fehler beim Lesen der Webcam.\")\n",
    "            break\n",
    "            \n",
    "        # Verarbeite Frame\n",
    "        frame = predictor.predict_frame(frame)\n",
    "        \n",
    "        # Zeige Frame\n",
    "        cv2.imshow('ASL Buchstaben-Erkennung', frame)\n",
    "        \n",
    "        # Beende bei 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Für Notebook-Verwendung\n",
    "class NotebookPredictor:\n",
    "    def __init__(self, model_path='/workspaces/asl_detection/machine_learning/models/asl_now/best_model.pth'):\n",
    "        self.predictor = ASLPredictor(model_path)\n",
    "        \n",
    "    def process_webcam(self, num_frames=100):\n",
    "        \"\"\"Verarbeitet eine bestimmte Anzahl von Frames aus der Webcam\"\"\"\n",
    "        from IPython.display import clear_output, Image, display\n",
    "        import PIL.Image\n",
    "        \n",
    "        # Öffne Webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Fehler: Webcam konnte nicht geöffnet werden\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            for _ in range(num_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Fehler beim Lesen der Webcam\")\n",
    "                    break\n",
    "                \n",
    "                # Verarbeite Frame\n",
    "                frame = self.predictor.predict_frame(frame)\n",
    "                \n",
    "                # Konvertiere zu RGB für Notebook-Anzeige\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Zeige Frame\n",
    "                clear_output(wait=True)\n",
    "                display(PIL.Image.fromarray(frame_rgb))\n",
    "                \n",
    "                # Kleine Pause für flüssigere Anzeige\n",
    "                time.sleep(0.1)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            \n",
    "        print(\"Erkennung abgeschlossen.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showcase-H-MG0u5T-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
