{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Define the model class (identical to training)\n",
    "class HandSignNet(nn.Module):\n",
    "    def __init__(self, num_classes=24):\n",
    "        super(HandSignNet, self).__init__()\n",
    "        \n",
    "        # Feature Extraction Blocks\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(63, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Landmark extraction for 63 features (exactly like in training)\n",
    "def extract_landmarks(results):\n",
    "    \"\"\"\n",
    "    Extract keypoints of the signing hand (right hand from the viewer's perspective)\n",
    "    Exact same method as in the training code\n",
    "    \"\"\"\n",
    "    # Initialize array for the signing hand (21 keypoints with x, y, z)\n",
    "    hand_keypoints = np.zeros(21 * 3)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # If multiple hands are detected, find the correct hand\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # The hand classification is from the camera's perspective\n",
    "            handedness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            if handedness == \"Right\":  # We are looking for the right hand from the camera's perspective\n",
    "                hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "                break\n",
    "        # If no right hand was found, take the first detected hand\n",
    "        if np.all(hand_keypoints == 0) and results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    return hand_keypoints\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Image is returned unchanged (no transformations)\n",
    "    \"\"\"\n",
    "    return image\n",
    "\n",
    "class ASLPredictor:\n",
    "    def __init__(self, model_path='/workspaces/asl_detection/machine_learning/models/asl_now/best_model.pth'):\n",
    "        # Initialize MediaPipe Hands (like in training)\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.2,\n",
    "            min_tracking_confidence=0.2)\n",
    "\n",
    "        # Load model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.model = HandSignNet().to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Letters mapping (all letters except j and z)\n",
    "        self.letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Buffer for more stable predictions\n",
    "        self.prediction_buffer = []\n",
    "        self.buffer_size = 5\n",
    "        self.last_prediction = \"\"\n",
    "\n",
    "    def predict_frame(self, frame):\n",
    "        \"\"\"Processes a frame and returns the prediction\"\"\"\n",
    "        # Preprocess the image (now without transformations)\n",
    "        frame = preprocess_image(frame)\n",
    "        \n",
    "        # Convert to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(frame_rgb)\n",
    "        \n",
    "        # Draw hand landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    hand_landmarks, \n",
    "                    self.mp_hands.HAND_CONNECTIONS,\n",
    "                    self.mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    self.mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        # Make a prediction when a hand is detected (exactly like in training)\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Extract features\n",
    "            landmarks = extract_landmarks(results)\n",
    "            \n",
    "            # Model prediction\n",
    "            with torch.no_grad():\n",
    "                landmarks_tensor = torch.FloatTensor(landmarks).unsqueeze(0).to(self.device)\n",
    "                outputs = self.model(landmarks_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, prediction = torch.max(probabilities, dim=1)\n",
    "                \n",
    "                # Get letter and confidence\n",
    "                predicted_letter = self.letters[prediction.item()]\n",
    "                confidence_value = confidence.item()\n",
    "                \n",
    "                # Add prediction to buffer\n",
    "                self.prediction_buffer.append(predicted_letter)\n",
    "                \n",
    "                # Limit buffer size\n",
    "                if len(self.prediction_buffer) > self.buffer_size:\n",
    "                    self.prediction_buffer.pop(0)\n",
    "                \n",
    "                # Select the most common prediction\n",
    "                if self.prediction_buffer:\n",
    "                    most_common = Counter(self.prediction_buffer).most_common(1)\n",
    "                    self.last_prediction = most_common[0][0]\n",
    "                    frequency = most_common[0][1] / len(self.prediction_buffer)\n",
    "                    \n",
    "                    # Display the prediction\n",
    "                    if frequency > 0.6:  # Only display if more than 60% of predictions match\n",
    "                        cv2.rectangle(frame, (0, 0), (200, 100), (245, 117, 16), -1)\n",
    "                        cv2.putText(frame, self.last_prediction.upper(), \n",
    "                                  (60, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame, f\"Conf: {confidence_value:.2f}\", \n",
    "                                  (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    # Initialize predictor\n",
    "    predictor = ASLPredictor()\n",
    "    \n",
    "    # Open webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    print(\"Starting real-time detection... (Press 'q' to exit)\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error reading webcam.\")\n",
    "            break\n",
    "            \n",
    "        # Process frame\n",
    "        frame = predictor.predict_frame(frame)\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow('ASL Letter Detection', frame)\n",
    "        \n",
    "        # Exit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# For notebook usage\n",
    "class NotebookPredictor:\n",
    "    def __init__(self, model_path='/workspaces/asl_detection/machine_learning/models/asl_now/best_model.pth'):\n",
    "        self.predictor = ASLPredictor(model_path)\n",
    "        \n",
    "    def process_webcam(self, num_frames=100):\n",
    "        \"\"\"Processes a specific number of frames from the webcam\"\"\"\n",
    "        from IPython.display import clear_output, Image, display\n",
    "        import PIL.Image\n",
    "        \n",
    "        # Open webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            for _ in range(num_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Error reading webcam\")\n",
    "                    break\n",
    "                \n",
    "                # Process frame\n",
    "                frame = self.predictor.predict_frame(frame)\n",
    "                \n",
    "                # Convert to RGB for notebook display\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Display frame\n",
    "                clear_output(wait=True)\n",
    "                display(PIL.Image.fromarray(frame_rgb))\n",
    "                \n",
    "                # Small pause for smoother display\n",
    "                time.sleep(0.1)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            \n",
    "        print(\"Detection completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showcase-H-MG0u5T-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
