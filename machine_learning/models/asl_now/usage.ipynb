{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "# MediaPipe Holistic initialisieren\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Konstanten\n",
    "LABELS = ['g', 'h', 'l', 'x']  # Die 4 Klassen des Modells\n",
    "\n",
    "# Modellklasse definieren (identisch zum Training)\n",
    "class HandSignNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(HandSignNet, self).__init__()\n",
    "        \n",
    "        # Feature Extraction Blocks\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(63, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Landmark-Extraktion für 63 Features\n",
    "def extract_landmarks(results):\n",
    "    # Rechte Hand-Keypoints (21 Landmarks x 3 Koordinaten = 63 Features)\n",
    "    if results.right_hand_landmarks:\n",
    "        # Extrahiere die 21 Handlandmarks mit xyz-Koordinaten\n",
    "        hand_landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark])\n",
    "        \n",
    "        # Normalisieren - relative Position zum Handgelenk (Landmark 0)\n",
    "        wrist = hand_landmarks[0].copy()\n",
    "        hand_landmarks = hand_landmarks - wrist\n",
    "        \n",
    "        # Flachdrücken zu einem 63D-Vektor\n",
    "        features = hand_landmarks.flatten()\n",
    "    else:\n",
    "        # Wenn keine Hand erkannt wird, gib einen Nullvektor zurück\n",
    "        features = np.zeros(63)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Modell laden\n",
    "def load_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Gerät: {device}\")\n",
    "    \n",
    "    # Modell initialisieren\n",
    "    model = HandSignNet()\n",
    "    \n",
    "    # Modell laden\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Modell erfolgreich geladen von: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden des Modells: {e}\")\n",
    "        return None\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Hauptfunktion\n",
    "def main():\n",
    "    # Modellpfad\n",
    "    model_path = \"/home/geiger/asl_detection/machine_learning/models/asl_now/best_model.pth\"\n",
    "    # Modell laden\n",
    "    model = load_model(model_path)\n",
    "    if model is None:\n",
    "        print(\"Konnte Modell nicht laden. Beende Programm.\")\n",
    "        return\n",
    "    \n",
    "    # Webcam initialisieren\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Text-Einstellungen\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.5\n",
    "    font_thickness = 2\n",
    "    \n",
    "    # Frame-Zähler und Puffer für stabilere Vorhersagen\n",
    "    frame_counter = 0\n",
    "    prediction_buffer = []\n",
    "    buffer_size = 5\n",
    "    last_prediction = \"\"\n",
    "    \n",
    "    # MediaPipe Holistic starten\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Fehler beim Erfassen des Frames\")\n",
    "                break\n",
    "            \n",
    "            # Frame für MediaPipe umwandeln\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            \n",
    "            # Keypoints erkennen\n",
    "            results = holistic.process(image)\n",
    "            \n",
    "            # Zurück zu BGR umwandeln\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Landmarks zeichnen (nur Hände, da unser Modell nur Handdaten verwendet)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "            )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "            \n",
    "            # Nur alle paar Frames eine Vorhersage machen\n",
    "            frame_counter += 1\n",
    "            if frame_counter % 5 == 0:  # Alle 5 Frames\n",
    "                if results.right_hand_landmarks:  # Wir verwenden nur die rechte Hand für die Vorhersage\n",
    "                    # Features extrahieren\n",
    "                    landmarks = extract_landmarks(results)\n",
    "                    \n",
    "                    # Modellvorhersage\n",
    "                    with torch.no_grad():\n",
    "                        landmarks_tensor = torch.FloatTensor(landmarks).unsqueeze(0)\n",
    "                        outputs = model(landmarks_tensor)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        \n",
    "                        # Vorhersage zum Puffer hinzufügen\n",
    "                        prediction_buffer.append(LABELS[predicted.item()])\n",
    "                        \n",
    "                        # Puffer-Größe begrenzen\n",
    "                        if len(prediction_buffer) > buffer_size:\n",
    "                            prediction_buffer.pop(0)\n",
    "                        \n",
    "                        # Häufigste Vorhersage auswählen\n",
    "                        if prediction_buffer:\n",
    "                            from collections import Counter\n",
    "                            last_prediction = Counter(prediction_buffer).most_common(1)[0][0]\n",
    "            \n",
    "            # Vorhersage anzeigen\n",
    "            cv2.rectangle(image, (0, 0), (200, 100), (245, 117, 16), -1)\n",
    "            cv2.putText(image, last_prediction, (60, 60), font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
    "            \n",
    "            # Ergebnis anzeigen\n",
    "            cv2.imshow('ASL Erkennung', image)\n",
    "            \n",
    "            # Abbruch bei 'q' drücken\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
