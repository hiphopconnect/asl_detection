{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutze Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741100913.223670 3495912 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1741100913.242532 4013526 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.216.01), renderer: NVIDIA RTX A6000/PCIe/SSE2\n",
      "W0000 00:00:1741100913.279937 4013496 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741100913.313113 4013521 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'machine_learning/models/asl_now/best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErkennung abgeschlossen.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[2], line 161\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Initialisiere Predictor\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[43mASLPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Öffne Webcam\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 88\u001b[0m, in \u001b[0;36mASLPredictor.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNutze Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m HandSignNet()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Buchstaben-Mapping (alle Buchstaben außer j und z)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'machine_learning/models/asl_now/best_model.pth'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Modellklasse definieren (identisch zum Training)\n",
    "class HandSignNet(nn.Module):\n",
    "    def __init__(self, num_classes=24):\n",
    "        super(HandSignNet, self).__init__()\n",
    "        \n",
    "        # Feature Extraction Blocks\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(63, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Landmark-Extraktion für 63 Features (exakt wie im Training)\n",
    "def extract_landmarks(results):\n",
    "    \"\"\"\n",
    "    Extrahiert Keypoints der signierenden Hand (rechte Hand aus Sicht des Betrachters)\n",
    "    Exakt gleiche Methode wie im Trainingscode\n",
    "    \"\"\"\n",
    "    # Initialisiere Array für die signierende Hand (21 Keypoints mit x, y, z)\n",
    "    hand_keypoints = np.zeros(21 * 3)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Wenn mehrere Hände erkannt wurden, finde die richtige Hand\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Die Hand-Klassifikation ist aus Sicht der Kamera\n",
    "            handedness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            if handedness == \"Right\":  # Wir suchen die rechte Hand aus Sicht der Kamera\n",
    "                hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "                break\n",
    "        # Falls keine rechte Hand gefunden wurde, nimm die erste erkannte Hand\n",
    "        if np.all(hand_keypoints == 0) and results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    return hand_keypoints\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Bild wird unverändert zurückgegeben (keine Transformationen)\n",
    "    \"\"\"\n",
    "    return image\n",
    "\n",
    "class ASLPredictor:\n",
    "    def __init__(self, model_path='machine_learning/models/asl_now/best_model.pth'):\n",
    "        # MediaPipe Hands initialisieren (wie beim Training)\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.2,\n",
    "            min_tracking_confidence=0.2)\n",
    "\n",
    "        # Modell laden\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Nutze Device: {self.device}\")\n",
    "        \n",
    "        self.model = HandSignNet().to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Buchstaben-Mapping (alle Buchstaben außer j und z)\n",
    "        self.letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
    "        print(\"Modell erfolgreich geladen!\")\n",
    "        \n",
    "        # Puffer für stabilere Vorhersagen\n",
    "        self.prediction_buffer = []\n",
    "        self.buffer_size = 5\n",
    "        self.last_prediction = \"\"\n",
    "\n",
    "    def predict_frame(self, frame):\n",
    "        \"\"\"Verarbeitet ein Frame und gibt die Vorhersage zurück\"\"\"\n",
    "        # Bild vorverarbeiten (jetzt ohne Transformationen)\n",
    "        frame = preprocess_image(frame)\n",
    "        \n",
    "        # Konvertiere zu RGB für MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands.process(frame_rgb)\n",
    "        \n",
    "        # Zeichne Handpunkte\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    hand_landmarks, \n",
    "                    self.mp_hands.HAND_CONNECTIONS,\n",
    "                    self.mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                    self.mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        # Mache Vorhersage wenn Hand erkannt wurde (exakt wie im Training)\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Features extrahieren\n",
    "            landmarks = extract_landmarks(results)\n",
    "            \n",
    "            # Modellvorhersage\n",
    "            with torch.no_grad():\n",
    "                landmarks_tensor = torch.FloatTensor(landmarks).unsqueeze(0).to(self.device)\n",
    "                outputs = self.model(landmarks_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, prediction = torch.max(probabilities, dim=1)\n",
    "                \n",
    "                # Hole Buchstaben und Konfidenz\n",
    "                predicted_letter = self.letters[prediction.item()]\n",
    "                confidence_value = confidence.item()\n",
    "                \n",
    "                # Vorhersage zum Puffer hinzufügen\n",
    "                self.prediction_buffer.append(predicted_letter)\n",
    "                \n",
    "                # Puffer-Größe begrenzen\n",
    "                if len(self.prediction_buffer) > self.buffer_size:\n",
    "                    self.prediction_buffer.pop(0)\n",
    "                \n",
    "                # Häufigste Vorhersage auswählen\n",
    "                if self.prediction_buffer:\n",
    "                    most_common = Counter(self.prediction_buffer).most_common(1)\n",
    "                    self.last_prediction = most_common[0][0]\n",
    "                    frequency = most_common[0][1] / len(self.prediction_buffer)\n",
    "                    \n",
    "                    # Zeige Vorhersage an\n",
    "                    if frequency > 0.6:  # Nur anzeigen wenn mehr als 60% der Vorhersagen übereinstimmen\n",
    "                        cv2.rectangle(frame, (0, 0), (200, 100), (245, 117, 16), -1)\n",
    "                        cv2.putText(frame, self.last_prediction.upper(), \n",
    "                                  (60, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame, f\"Konf: {confidence_value:.2f}\", \n",
    "                                  (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "def main():\n",
    "    # Initialisiere Predictor\n",
    "    predictor = ASLPredictor()\n",
    "    \n",
    "    # Öffne Webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    print(\"Starte Echtzeit-Erkennung... (Drücke 'q' zum Beenden)\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Fehler beim Lesen der Webcam.\")\n",
    "            break\n",
    "            \n",
    "        # Verarbeite Frame\n",
    "        frame = predictor.predict_frame(frame)\n",
    "        \n",
    "        # Zeige Frame\n",
    "        cv2.imshow('ASL Buchstaben-Erkennung', frame)\n",
    "        \n",
    "        # Beende bei 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Für Notebook-Verwendung\n",
    "class NotebookPredictor:\n",
    "    def __init__(self, model_path='machine_learning/models/asl_now/best_model.pth'):\n",
    "        self.predictor = ASLPredictor(model_path)\n",
    "        \n",
    "    def process_webcam(self, num_frames=100):\n",
    "        \"\"\"Verarbeitet eine bestimmte Anzahl von Frames aus der Webcam\"\"\"\n",
    "        from IPython.display import clear_output, Image, display\n",
    "        import PIL.Image\n",
    "        \n",
    "        # Öffne Webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Fehler: Webcam konnte nicht geöffnet werden\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            for _ in range(num_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Fehler beim Lesen der Webcam\")\n",
    "                    break\n",
    "                \n",
    "                # Verarbeite Frame\n",
    "                frame = self.predictor.predict_frame(frame)\n",
    "                \n",
    "                # Konvertiere zu RGB für Notebook-Anzeige\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Zeige Frame\n",
    "                clear_output(wait=True)\n",
    "                display(PIL.Image.fromarray(frame_rgb))\n",
    "                \n",
    "                # Kleine Pause für flüssigere Anzeige\n",
    "                time.sleep(0.1)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            \n",
    "        print(\"Erkennung abgeschlossen.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showcase-H-MG0u5T-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
