{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ordnerpfad anpassen\n",
    "folder_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets/own_dataset/keypoints\" \n",
    "\n",
    "# Alle JSON-Dateien im Ordner finden\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "print(\"Gefundene JSON-Dateien:\", json_files)\n",
    "print(\"Anzahl geladener JSON-Dateien:\", len(json_files))\n",
    "\n",
    "# Laden aller JSON-Dateien in eine Liste\n",
    "all_data = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Definiere die relevanten Teile\n",
    "parts = ['pose', 'face', 'left_hand', 'right_hand']\n",
    "\n",
    "# Funktion, um die maximale Länge für einen Part zu ermitteln\n",
    "def get_max_length(keypoints, part):\n",
    "    lengths = []\n",
    "    for kp in keypoints:\n",
    "        if kp.get(part) is not None and len(kp[part]) > 0:\n",
    "            lengths.append(np.array(kp[part]).flatten().shape[0])\n",
    "    return max(lengths) if lengths else 0\n",
    "\n",
    "# Bestimme die globalen maximalen Längen für jeden Part über alle Dateien\n",
    "global_expected = {part: 0 for part in parts}\n",
    "for data in all_data:\n",
    "    if data.get(\"keypoints\"):\n",
    "        for part in parts:\n",
    "            max_len = get_max_length(data[\"keypoints\"], part)\n",
    "            if max_len > global_expected[part]:\n",
    "                global_expected[part] = max_len\n",
    "print(\"Global expected lengths:\", global_expected)\n",
    "\n",
    "# Funktion zur Feature-Extraktion aus den Keypoints\n",
    "def extract_features(keypoints, expected_lengths):\n",
    "    features = []\n",
    "    for kp in keypoints:\n",
    "        frame_features = []\n",
    "        for part in parts:\n",
    "            if kp.get(part) is not None and len(kp[part]) > 0:\n",
    "                vals = np.array(kp[part]).flatten().tolist()\n",
    "                frame_features.extend(vals)\n",
    "            else:\n",
    "                frame_features.extend([0] * expected_lengths[part])\n",
    "        features.append(frame_features)\n",
    "    return np.array(features)\n",
    "\n",
    "# Erstelle eine Liste von Tensoren (eine Sequenz pro JSON) und speichere die zugehörigen Labels\n",
    "feature_list = []\n",
    "filtered_labels = []\n",
    "for data in all_data:\n",
    "    if not data.get(\"keypoints\"):\n",
    "        continue\n",
    "    features = extract_features(data[\"keypoints\"], global_expected)\n",
    "    if features.size == 0:\n",
    "        continue\n",
    "    tensor_feat = torch.tensor(features, dtype=torch.float32)\n",
    "    feature_list.append(tensor_feat)\n",
    "    filtered_labels.append(data[\"gloss\"])\n",
    "\n",
    "if not feature_list:\n",
    "    raise ValueError(\"Keine gültigen Feature-Tensoren gefunden!\")\n",
    "\n",
    "# Padding: Alle Sequenzen auf gleiche Timesteps-Länge bringen\n",
    "X_tensor = pad_sequence(feature_list, batch_first=True)\n",
    "print(\"Shape der gepaddeten Features:\", X_tensor.shape)\n",
    "\n",
    "# Normalisierung: Über alle Elemente\n",
    "mean = X_tensor.mean()\n",
    "std = X_tensor.std() + 1e-5  # Vermeidung von Division durch Null\n",
    "X_tensor = (X_tensor - mean) / std\n",
    "print(\"Features normalisiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Label-Encoding\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(filtered_labels)\n",
    "y_tensor = torch.tensor(labels_encoded, dtype=torch.long)\n",
    "print(\"Enkodierte Labels:\", labels_encoded)\n",
    "print(\"Anzahl Klassen:\", len(le.classes_))\n",
    "\n",
    "# Stratifizierte Aufteilung: Testgröße so wählen, dass jede Klasse mindestens vertreten ist\n",
    "# Bei 15 Samples und 5 Klassen verwenden wir beispielsweise 33% (ca. 5 Samples) für Validierung\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.33, random_state=42, stratify=y_tensor\n",
    ")\n",
    "print(\"Train Samples:\", X_train.shape[0], \"Validation Samples:\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, timesteps, features)\n",
    "        out, _ = self.lstm(x)  # out: (batch, timesteps, hidden_size)\n",
    "        out = out[:, -1, :]    # Letzter Zeitschritt\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_size = X_tensor.shape[2]\n",
    "hidden_size = 32  # Reduzierte Hidden Size\n",
    "num_classes = len(le.classes_)  # Sollte 5 ergeben\n",
    "model = SimpleLSTMClassifier(input_size, hidden_size, num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 200\n",
    "patience = 10  # Frühstopp, falls der Validierungsloss 10 Epochen lang nicht sinkt\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation im Trainings- und Validierungsmodus\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred = model(X_train)\n",
    "        train_loss = criterion(train_pred, y_train)\n",
    "        train_acc = (torch.argmax(train_pred, dim=1) == y_train).float().mean().item()\n",
    "        \n",
    "        val_pred = model(X_val)\n",
    "        val_loss = criterion(val_pred, y_val)\n",
    "        val_acc = (torch.argmax(val_pred, dim=1) == y_val).float().mean().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:03d}: Train Loss {train_loss.item():.4f}, Train Acc {train_acc*100:.2f}%, \"\n",
    "          f\"Val Loss {val_loss.item():.4f}, Val Acc {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Laden des besten Modells\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_val)\n",
    "    predicted_classes = torch.argmax(pred, dim=1).numpy()\n",
    "    predicted_labels = le.inverse_transform(predicted_classes)\n",
    "    print(\"Vorhersage auf dem Validierungssatz:\", predicted_labels)\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"lstm_model.pth\")\n",
    "print(\"Modell gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
