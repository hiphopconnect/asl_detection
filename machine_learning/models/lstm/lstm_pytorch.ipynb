{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training via Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 1. Load Data -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/keypoints\"\n",
    "train_json_dir = os.path.join(data_dir, \"train/openpose_output/json\")\n",
    "val_json_dir = os.path.join(data_dir, \"val/openpose_output/json\")\n",
    "test_json_dir = os.path.join(data_dir, \"test/openpose_output/json\")\n",
    "\n",
    "csv_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "train_labels_csv = os.path.join(csv_path, \"how2sign_realigned_train.csv\")\n",
    "val_labels_csv = os.path.join(csv_path, \"how2sign_realigned_val.csv\")\n",
    "test_labels_csv = os.path.join(csv_path, \"how2sign_realigned_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ----- Training Labels -----\n",
    "train_label_df = pd.read_csv(train_labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values for training\n",
    "unique_train_sentences = sorted(set(train_label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed IDs for training\n",
    "sentence_to_id_train = {sentence: idx for idx, sentence in enumerate(unique_train_sentences)}\n",
    "\n",
    "# Create mapping: SENTENCE_NAME -> 0-indexed SENTENCE_ID for training\n",
    "label_mapping_train = {\n",
    "    name: sentence_id\n",
    "    for name, sentence_id in zip(train_label_df[\"SENTENCE_NAME\"], train_label_df[\"SENTENCE_ID\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping\n",
    "y_labels = [sentence_to_id_train[sentence_id] for sentence_id in train_label_df[\"SENTENCE_ID\"]]\n",
    "print(f\"New y_labels min: {min(y_labels)}, max: {max(y_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Validation Labels -----\n",
    "val_label_df = pd.read_csv(val_labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values for validation\n",
    "unique_val_sentences = sorted(set(val_label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed IDs for validation\n",
    "sentence_to_id_val = {sentence: idx for idx, sentence in enumerate(unique_val_sentences)}\n",
    "\n",
    "# Create mapping: SENTENCE_NAME -> 0-indexed SENTENCE_ID for validation\n",
    "label_mapping_val = {\n",
    "    name: sentence_id\n",
    "    for name, sentence_id in zip(val_label_df[\"SENTENCE_NAME\"], val_label_df[\"SENTENCE_ID\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Test Labels -----\n",
    "test_label_df = pd.read_csv(test_labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values for test\n",
    "unique_test_sentences = sorted(set(test_label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed IDs for test\n",
    "sentence_to_id_test = {sentence: idx for idx, sentence in enumerate(unique_test_sentences)}\n",
    "\n",
    "# Create mapping: SENTENCE_NAME -> 0-indexed SENTENCE_ID for test\n",
    "label_mapping_test = {\n",
    "    name: sentence_id\n",
    "    for name, sentence_id in zip(test_label_df[\"SENTENCE_NAME\"], test_label_df[\"SENTENCE_ID\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print examples to check the mappings\n",
    "print(\"Train mapping example:\", list(label_mapping_train.items())[:5])\n",
    "print(\"Val mapping example:\", list(label_mapping_val.items())[:5])\n",
    "print(\"Test mapping example:\", list(label_mapping_test.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_keypoints(json_folder, max_frames=100):\n",
    "    \"\"\"\n",
    "    Load keypoints from JSON files and return a padded sequence as a tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (max_frames, feature_dim)\n",
    "    \"\"\"\n",
    "    keypoints_sequence = []\n",
    "    required_dim = 411  # Fixed feature dimension\n",
    "\n",
    "    for frame_file in sorted(os.listdir(json_folder)):\n",
    "        frame_path = os.path.join(json_folder, frame_file)\n",
    "        with open(frame_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if \"people\" in data and len(data[\"people\"]) > 0:\n",
    "            person = data[\"people\"][0]  # First detected person\n",
    "\n",
    "            # Extract keypoints from different parts\n",
    "            pose = person.get(\"pose_keypoints_2d\", [])\n",
    "            face = person.get(\"face_keypoints_2d\", [])\n",
    "            left_hand = person.get(\"hand_left_keypoints_2d\", [])\n",
    "            right_hand = person.get(\"hand_right_keypoints_2d\", [])\n",
    "            \n",
    "            # Combine all keypoints\n",
    "            full_keypoints = pose + face + left_hand + right_hand\n",
    "            \n",
    "            # Pad or truncate to required_dim\n",
    "            if len(full_keypoints) < required_dim:\n",
    "                full_keypoints += [0.0] * (required_dim - len(full_keypoints))\n",
    "            else:\n",
    "                full_keypoints = full_keypoints[:required_dim]\n",
    "            \n",
    "            keypoints_tensor = torch.tensor(full_keypoints, dtype=torch.float32)\n",
    "            keypoints_sequence.append(keypoints_tensor)\n",
    "    \n",
    "    # If no frames were loaded, return zeros\n",
    "    if not keypoints_sequence:\n",
    "        return torch.zeros((max_frames, required_dim), dtype=torch.float32)\n",
    "    \n",
    "    # Stack tensors: (num_frames, feature_dim)\n",
    "    seq_tensor = torch.stack(keypoints_sequence)\n",
    "    \n",
    "    # Pad or truncate to max_frames\n",
    "    if seq_tensor.shape[0] < max_frames:\n",
    "        padded_sequence = torch.zeros((max_frames, required_dim), dtype=torch.float32)\n",
    "        padded_sequence[:seq_tensor.shape[0]] = seq_tensor\n",
    "    else:\n",
    "        padded_sequence = seq_tensor[:max_frames]\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Maximum number of frames per sequence (set based on dataset analysis)\n",
    "MAX_FRAMES = 200  \n",
    "\n",
    "def pad_or_truncate(sequence, max_frames=MAX_FRAMES):\n",
    "    \"\"\"Pads or truncates the sequence tensor to ensure a fixed length.\"\"\"\n",
    "    num_frames, num_features = sequence.shape\n",
    "    if num_frames < max_frames:\n",
    "        pad = torch.zeros((max_frames - num_frames, num_features),\n",
    "                          dtype=sequence.dtype, device=sequence.device)\n",
    "        sequence = torch.cat((sequence, pad), dim=0)\n",
    "    else:\n",
    "        sequence = sequence[:max_frames]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(json_dir, mapping, sentence_to_id, max_frames=MAX_FRAMES):\n",
    "    X_data, y_labels = [], []\n",
    "\n",
    "    print(f\"\\nChecking JSON directory: {json_dir}\")\n",
    "    json_folders = os.listdir(json_dir)\n",
    "    print(f\"Existing JSON folders: {json_folders[:5]}\")\n",
    "\n",
    "    for folder_name in json_folders:\n",
    "        folder_path = os.path.join(json_dir, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Skipping '{folder_name}' (not a directory)\")\n",
    "            continue\n",
    "\n",
    "        # Check if folder name exists in mapping\n",
    "        if folder_name not in mapping:\n",
    "            print(f\"Skipping: '{folder_name}' (not in mapping)\")\n",
    "            continue\n",
    "\n",
    "        # Get the sentence ID directly (String)\n",
    "        sentence_id = mapping[folder_name]\n",
    "\n",
    "        # Check if sentence ID exists in sentence_to_id\n",
    "        if sentence_id not in sentence_to_id:\n",
    "            print(f\"Skipping: Sentence ID '{sentence_id}' (not in sentence_to_id)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing: '{folder_name}' -> Sentence ID '{sentence_id}' -> Mapped ID {sentence_to_id[sentence_id]}\")\n",
    "\n",
    "        # Load keypoints and normalize\n",
    "        keypoints_sequence = load_keypoints(folder_path)\n",
    "        keypoints_sequence = pad_or_truncate(keypoints_sequence, max_frames)\n",
    "\n",
    "        X_data.append(keypoints_sequence)\n",
    "        y_labels.append(sentence_to_id[sentence_id])\n",
    "\n",
    "    if not X_data:\n",
    "        print(f\"\\n⚠️  No valid data found in {json_dir} ⚠️\")\n",
    "\n",
    "    X_data = torch.stack(X_data) if X_data else torch.empty(0, max_frames, 411)\n",
    "    y_labels = torch.tensor(y_labels, dtype=torch.long) if y_labels else torch.empty(0, dtype=torch.long)\n",
    "    \n",
    "    return X_data, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sentence_to_id example:\", list(sentence_to_id_train.keys())[:5])\n",
    "print(\"label_mapping values:\", list(label_mapping_train.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 2. Prepare Data for PyTorch -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for training\n",
    "X_train, y_train = process_data(train_json_dir, label_mapping_train, sentence_to_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train samples:\", X_train.shape[0])\n",
    "print(\"y_train samples:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for validation \n",
    "X_val, y_val = process_data(val_json_dir, label_mapping_val, sentence_to_id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for testing\n",
    "X_test, y_test = process_data(test_json_dir, label_mapping_test, sentence_to_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batch processing\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 3. Define LSTM Model -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define an LSTM-based model for sequence classification\n",
    "class SignLanguageLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=411, hidden_dim=1024, num_layers=2, output_dim=30814, dropout=0.5):\n",
    "        super(SignLanguageLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Dropout check (PyTorch does not allow dropout for a single-layer LSTM)\n",
    "        dropout = dropout if num_layers > 1 else 0\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        if lengths is not None:\n",
    "            # Pack the sequence to ignore padded frames\n",
    "            packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "            packed_output, _ = self.lstm(packed_input)\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            # Extract the last valid output for each sequence\n",
    "            if lengths is not None:\n",
    "                last_outputs = torch.stack([lstm_out[i, length-1, :] for i, length in enumerate(lengths)])\n",
    "            else:\n",
    "                last_outputs = lstm_out[:, -1, :]\n",
    "        else:\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            last_outputs = lstm_out[:, -1, :]\n",
    "            \n",
    "        return self.fc(last_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 411            # Number of keypoints per frame\n",
    "hidden_dim = 1024           # Number of hidden units in LSTM\n",
    "num_layers = 2             # Number of LSTM layers\n",
    "output_dim = len(set(label_mapping_train))  # Number of classes (0-indexed)\n",
    "dropout = 0.5              # Dropout for regularization\n",
    "\n",
    "# Optionally disable cuDNN for debugging purposes\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# Create model and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device: \", device)\n",
    "# device = torch.device(\"cpu\")\n",
    "model = SignLanguageLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "\n",
    "# Apply weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "model.apply(init_weights)  #Apply to all layers\n",
    "\n",
    "# Define loss function (CrossEntropyLoss for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (Adam works well for LSTMs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "# Initialize LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single batch before training\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "print(\"y_batch min:\", y_batch.min().item(), \"y_batch max:\", y_batch.max().item())\n",
    "outputs = model(X_batch)\n",
    "print(\"Output shape:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 4. Training -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "num_epochs = 10  # Adjust as needed\n",
    "patience = 10   # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(f\"y_batch min: {y_batch.min().item()}, y_batch max: {y_batch.max().item()}\")\n",
    "        # print(f\"Output shape: {outputs.shape}, Output min: {outputs.min().item()}, Output max: {outputs.max().item()}\")\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validierungsschleife (vorausgesetzt, du hast einen val_loader)\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs_val = model(X_val)\n",
    "            loss_val = criterion(outputs_val, y_val)\n",
    "            total_val_loss += loss_val.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # LR-Scheduler: Update based on Validation Loss\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early Stopping: Prüfe, ob sich der Validierungs-Loss verbessert hat\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 5. Save Model & Evaluate -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Save only state_dict\n",
    "torch.save(model.state_dict(), \"sign_language_lstm_state.pth\")\n",
    "\n",
    "# Then later load it into a model instance\n",
    "model = SignLanguageLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "model.load_state_dict(torch.load(\"sign_language_lstm_state.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 6. Testing & Inference -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample_input):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Ensure input is a PyTorch tensor\n",
    "    if not isinstance(sample_input, torch.Tensor):\n",
    "        sample_input = torch.tensor(sample_input, dtype=torch.float32)\n",
    "    \n",
    "    sample_input = sample_input.unsqueeze(0).to(device)  # Add batch dimension and move to correct device\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = model(sample_input)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Example usage with test data\n",
    "# Ensure that X_test is not empty; here we take the first sample\n",
    "sample_idx = 0  # or any valid index in the test set\n",
    "sample_input = X_test[sample_idx]\n",
    "predicted_label = predict(model, sample_input)\n",
    "print(f\"Predicted Label (Test Data): {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
