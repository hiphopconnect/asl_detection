{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 5.3631, Accuracy: 0.67%\n",
      "✅ Best model saved at epoch 1 with loss 5.3631\n",
      "Epoch [2/200], Loss: 5.3167, Accuracy: 0.96%\n",
      "✅ Best model saved at epoch 2 with loss 5.3167\n",
      "Epoch [3/200], Loss: 5.2880, Accuracy: 1.26%\n",
      "✅ Best model saved at epoch 3 with loss 5.2880\n",
      "Epoch [4/200], Loss: 5.1410, Accuracy: 1.50%\n",
      "✅ Best model saved at epoch 4 with loss 5.1410\n",
      "Epoch [5/200], Loss: 5.0037, Accuracy: 1.95%\n",
      "✅ Best model saved at epoch 5 with loss 5.0037\n",
      "Epoch [6/200], Loss: 4.9226, Accuracy: 2.66%\n",
      "✅ Best model saved at epoch 6 with loss 4.9226\n",
      "Epoch [7/200], Loss: 4.8310, Accuracy: 3.59%\n",
      "✅ Best model saved at epoch 7 with loss 4.8310\n",
      "Epoch [8/200], Loss: 4.6754, Accuracy: 4.68%\n",
      "✅ Best model saved at epoch 8 with loss 4.6754\n",
      "Epoch [9/200], Loss: 4.4711, Accuracy: 6.59%\n",
      "✅ Best model saved at epoch 9 with loss 4.4711\n",
      "Epoch [10/200], Loss: 4.3216, Accuracy: 8.04%\n",
      "✅ Best model saved at epoch 10 with loss 4.3216\n",
      "Epoch [11/200], Loss: 4.2373, Accuracy: 9.46%\n",
      "✅ Best model saved at epoch 11 with loss 4.2373\n",
      "Epoch [12/200], Loss: 4.1610, Accuracy: 11.12%\n",
      "✅ Best model saved at epoch 12 with loss 4.1610\n",
      "Epoch [13/200], Loss: 4.0925, Accuracy: 12.22%\n",
      "✅ Best model saved at epoch 13 with loss 4.0925\n",
      "Epoch [14/200], Loss: 4.0262, Accuracy: 12.87%\n",
      "✅ Best model saved at epoch 14 with loss 4.0262\n",
      "Epoch [15/200], Loss: 3.9457, Accuracy: 14.88%\n",
      "✅ Best model saved at epoch 15 with loss 3.9457\n",
      "Epoch [16/200], Loss: 3.8549, Accuracy: 16.84%\n",
      "✅ Best model saved at epoch 16 with loss 3.8549\n",
      "Epoch [17/200], Loss: 3.7860, Accuracy: 17.97%\n",
      "✅ Best model saved at epoch 17 with loss 3.7860\n",
      "Epoch [18/200], Loss: 3.6887, Accuracy: 20.66%\n",
      "✅ Best model saved at epoch 18 with loss 3.6887\n",
      "Epoch [19/200], Loss: 3.6021, Accuracy: 22.31%\n",
      "✅ Best model saved at epoch 19 with loss 3.6021\n",
      "Epoch [20/200], Loss: 3.5175, Accuracy: 24.83%\n",
      "✅ Best model saved at epoch 20 with loss 3.5175\n",
      "Epoch [21/200], Loss: 3.4096, Accuracy: 28.31%\n",
      "✅ Best model saved at epoch 21 with loss 3.4096\n",
      "Epoch [22/200], Loss: 3.3099, Accuracy: 31.02%\n",
      "✅ Best model saved at epoch 22 with loss 3.3099\n",
      "Epoch [23/200], Loss: 3.2060, Accuracy: 33.85%\n",
      "✅ Best model saved at epoch 23 with loss 3.2060\n",
      "Epoch [24/200], Loss: 3.1213, Accuracy: 36.48%\n",
      "✅ Best model saved at epoch 24 with loss 3.1213\n",
      "Epoch [25/200], Loss: 3.0342, Accuracy: 38.66%\n",
      "✅ Best model saved at epoch 25 with loss 3.0342\n",
      "Epoch [26/200], Loss: 2.9378, Accuracy: 41.47%\n",
      "✅ Best model saved at epoch 26 with loss 2.9378\n",
      "Epoch [27/200], Loss: 2.8397, Accuracy: 44.52%\n",
      "✅ Best model saved at epoch 27 with loss 2.8397\n",
      "Epoch [28/200], Loss: 2.7291, Accuracy: 48.03%\n",
      "✅ Best model saved at epoch 28 with loss 2.7291\n",
      "Epoch [29/200], Loss: 2.6331, Accuracy: 50.98%\n",
      "✅ Best model saved at epoch 29 with loss 2.6331\n",
      "Epoch [30/200], Loss: 2.5578, Accuracy: 53.27%\n",
      "✅ Best model saved at epoch 30 with loss 2.5578\n",
      "Epoch [31/200], Loss: 2.4457, Accuracy: 58.03%\n",
      "✅ Best model saved at epoch 31 with loss 2.4457\n",
      "Epoch [32/200], Loss: 2.3524, Accuracy: 61.39%\n",
      "✅ Best model saved at epoch 32 with loss 2.3524\n",
      "Epoch [33/200], Loss: 2.2681, Accuracy: 64.34%\n",
      "✅ Best model saved at epoch 33 with loss 2.2681\n",
      "Epoch [34/200], Loss: 2.1732, Accuracy: 67.12%\n",
      "✅ Best model saved at epoch 34 with loss 2.1732\n",
      "Epoch [35/200], Loss: 2.1039, Accuracy: 69.38%\n",
      "✅ Best model saved at epoch 35 with loss 2.1039\n",
      "Epoch [36/200], Loss: 2.0242, Accuracy: 72.20%\n",
      "✅ Best model saved at epoch 36 with loss 2.0242\n",
      "Epoch [37/200], Loss: 1.9409, Accuracy: 75.63%\n",
      "✅ Best model saved at epoch 37 with loss 1.9409\n",
      "Epoch [38/200], Loss: 1.8809, Accuracy: 77.54%\n",
      "✅ Best model saved at epoch 38 with loss 1.8809\n",
      "Epoch [39/200], Loss: 1.8099, Accuracy: 80.08%\n",
      "✅ Best model saved at epoch 39 with loss 1.8099\n",
      "Epoch [40/200], Loss: 1.7791, Accuracy: 81.10%\n",
      "✅ Best model saved at epoch 40 with loss 1.7791\n",
      "Epoch [41/200], Loss: 1.7136, Accuracy: 82.92%\n",
      "✅ Best model saved at epoch 41 with loss 1.7136\n",
      "Epoch [42/200], Loss: 1.6784, Accuracy: 84.55%\n",
      "✅ Best model saved at epoch 42 with loss 1.6784\n",
      "Epoch [43/200], Loss: 1.6475, Accuracy: 85.24%\n",
      "✅ Best model saved at epoch 43 with loss 1.6475\n",
      "Epoch [44/200], Loss: 1.5853, Accuracy: 87.75%\n",
      "✅ Best model saved at epoch 44 with loss 1.5853\n",
      "Epoch [45/200], Loss: 1.5482, Accuracy: 89.09%\n",
      "✅ Best model saved at epoch 45 with loss 1.5482\n",
      "Epoch [46/200], Loss: 1.5020, Accuracy: 90.39%\n",
      "✅ Best model saved at epoch 46 with loss 1.5020\n",
      "Epoch [47/200], Loss: 1.4884, Accuracy: 90.33%\n",
      "✅ Best model saved at epoch 47 with loss 1.4884\n",
      "Epoch [48/200], Loss: 1.4515, Accuracy: 91.46%\n",
      "✅ Best model saved at epoch 48 with loss 1.4515\n",
      "Epoch [49/200], Loss: 1.4390, Accuracy: 91.79%\n",
      "✅ Best model saved at epoch 49 with loss 1.4390\n",
      "Epoch [50/200], Loss: 1.4081, Accuracy: 92.00%\n",
      "✅ Best model saved at epoch 50 with loss 1.4081\n",
      "Epoch [51/200], Loss: 1.3743, Accuracy: 93.34%\n",
      "✅ Best model saved at epoch 51 with loss 1.3743\n",
      "Epoch [52/200], Loss: 1.3614, Accuracy: 93.04%\n",
      "✅ Best model saved at epoch 52 with loss 1.3614\n",
      "Epoch [53/200], Loss: 1.3016, Accuracy: 94.81%\n",
      "✅ Best model saved at epoch 53 with loss 1.3016\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Paths\n",
    "LABELS_PATH = \"/home/haggenmueller/asl_detection/machine_learning/models/lstm/label_to_index.json\"\n",
    "DATA_DIR = \"/home/haggenmueller/asl_detection/machine_learning/datasets/own_dataset/keypoints_npy\"\n",
    "MODEL_PATH = \"/home/haggenmueller/asl_detection/machine_learning/models/lstm/best_lstm_model.pth\"\n",
    "\n",
    "# Parameters\n",
    "SEQUENCE_LENGTH = 102  # Number of frames per sequence\n",
    "INPUT_SIZE = 300  # Number of extracted keypoints (adjusted based on feature extraction)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "LR = 0.0001\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "\n",
    "# Load labels\n",
    "with open(LABELS_PATH, \"r\") as f:\n",
    "    label_to_index = json.load(f)\n",
    "    index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "    NUM_CLASSES = len(label_to_index)\n",
    "\n",
    "# Dataset class\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_map, sequence_length):\n",
    "        self.data_dir = data_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.samples = []\n",
    "        \n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                filename_parts = file.split(\"_\")\n",
    "                label_name = filename_parts[1]  # Extract label from filename structure\n",
    "                if label_name in labels_map:\n",
    "                    self.samples.append((os.path.join(data_dir, file), labels_map[label_name]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        keypoints = np.load(file_path)\n",
    "        \n",
    "        # Ignore face keypoints (first 1872 values)\n",
    "        keypoints = keypoints[:, 468 * 4:]  # 4 values per keypoint (x, y, z, visibility)\n",
    "        \n",
    "        # Ensure all sequences are of the same length\n",
    "        if keypoints.shape[0] < self.sequence_length:\n",
    "            pad = np.zeros((self.sequence_length - keypoints.shape[0], keypoints.shape[1]))\n",
    "            keypoints = np.vstack((keypoints, pad))\n",
    "        else:\n",
    "            keypoints = keypoints[:self.sequence_length]\n",
    "        \n",
    "        return torch.tensor(keypoints, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "# LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)  # Batch Normalization\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])  # Apply Batch Normalization\n",
    "        out = self.dropout(out)  # Apply Dropout\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Load data\n",
    "dataset = ASLDataset(DATA_DIR, label_to_index, SEQUENCE_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMModel(input_size=INPUT_SIZE, hidden_size=512, num_layers=3, num_classes=NUM_CLASSES)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Apply Xavier Initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Early stopping\n",
    "best_loss = float(\"inf\")\n",
    "stopping_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for keypoints, labels in dataloader:\n",
    "        keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(keypoints)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / float(total)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Save best model if it improves\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        stopping_counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"✅ Best model saved at epoch {epoch+1} with loss {avg_loss:.4f}\")\n",
    "    else:\n",
    "        stopping_counter += 1\n",
    "        if stopping_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Training completed! Best model saved at:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.19856459330144"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for keypoints, labels in dataloader:\n",
    "            keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "            outputs = model(keypoints)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "evaluate_model(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
