{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate informations for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def compute_train_stats(folder_path):\n",
    "    \"\"\"Loads all .npy files in the folder and calculates the mean and standard deviation.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".npy\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = np.load(file_path)\n",
    "\n",
    "            if data.ndim == 1:  \n",
    "                data = data.reshape(1, -1)\n",
    "\n",
    "            all_data.append(data)\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No .npy files found in the folder!\")\n",
    "\n",
    "    all_data = np.vstack(all_data)\n",
    "\n",
    "    train_mean = np.mean(all_data, axis=0)\n",
    "    train_std = np.std(all_data, axis=0)\n",
    "\n",
    "    # Speicherort neben dem \"keypoints\" Ordner\n",
    "    parent_folder = os.path.dirname(folder_path)\n",
    "\n",
    "    np.save(os.path.join(parent_folder, \"train_mean.npy\"), train_mean)\n",
    "    np.save(os.path.join(parent_folder, \"train_std.npy\"), train_std)\n",
    "\n",
    "    print(\"✅ Mean & standard deviation saved next to the keypoints folder!\")\n",
    "\n",
    "# Example call:\n",
    "compute_train_stats(\"/home/haggenmueller/asl_detection/machine_learning/datasets/own_dataset/keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 5.0444, Accuracy: 1.69%\n",
      "✅ Best model saved at epoch 1 with loss 5.0444\n",
      "Epoch [2/200], Loss: 4.6239, Accuracy: 4.24%\n",
      "✅ Best model saved at epoch 2 with loss 4.6239\n",
      "Epoch [3/200], Loss: 4.4529, Accuracy: 5.93%\n",
      "✅ Best model saved at epoch 3 with loss 4.4529\n",
      "Epoch [4/200], Loss: 4.2901, Accuracy: 7.78%\n",
      "✅ Best model saved at epoch 4 with loss 4.2901\n",
      "Epoch [5/200], Loss: 4.1312, Accuracy: 10.03%\n",
      "✅ Best model saved at epoch 5 with loss 4.1312\n",
      "Epoch [6/200], Loss: 3.9891, Accuracy: 11.74%\n",
      "✅ Best model saved at epoch 6 with loss 3.9891\n",
      "Epoch [7/200], Loss: 3.8821, Accuracy: 13.65%\n",
      "✅ Best model saved at epoch 7 with loss 3.8821\n",
      "Epoch [8/200], Loss: 3.7913, Accuracy: 15.02%\n",
      "✅ Best model saved at epoch 8 with loss 3.7913\n",
      "Epoch [9/200], Loss: 3.7178, Accuracy: 18.10%\n",
      "✅ Best model saved at epoch 9 with loss 3.7178\n",
      "Epoch [10/200], Loss: 3.6182, Accuracy: 19.21%\n",
      "✅ Best model saved at epoch 10 with loss 3.6182\n",
      "Epoch [11/200], Loss: 3.5275, Accuracy: 21.57%\n",
      "✅ Best model saved at epoch 11 with loss 3.5275\n",
      "Epoch [12/200], Loss: 3.4476, Accuracy: 22.98%\n",
      "✅ Best model saved at epoch 12 with loss 3.4476\n",
      "Epoch [13/200], Loss: 3.3837, Accuracy: 24.33%\n",
      "✅ Best model saved at epoch 13 with loss 3.3837\n",
      "Epoch [14/200], Loss: 3.3198, Accuracy: 26.35%\n",
      "✅ Best model saved at epoch 14 with loss 3.3198\n",
      "Epoch [15/200], Loss: 3.2518, Accuracy: 27.34%\n",
      "✅ Best model saved at epoch 15 with loss 3.2518\n",
      "Epoch [16/200], Loss: 3.1733, Accuracy: 30.02%\n",
      "✅ Best model saved at epoch 16 with loss 3.1733\n",
      "Epoch [17/200], Loss: 3.1557, Accuracy: 30.72%\n",
      "✅ Best model saved at epoch 17 with loss 3.1557\n",
      "Epoch [18/200], Loss: 3.0714, Accuracy: 32.86%\n",
      "✅ Best model saved at epoch 18 with loss 3.0714\n",
      "Epoch [19/200], Loss: 3.0090, Accuracy: 34.28%\n",
      "✅ Best model saved at epoch 19 with loss 3.0090\n",
      "Epoch [20/200], Loss: 2.9257, Accuracy: 37.42%\n",
      "✅ Best model saved at epoch 20 with loss 2.9257\n",
      "Epoch [21/200], Loss: 2.8691, Accuracy: 39.23%\n",
      "✅ Best model saved at epoch 21 with loss 2.8691\n",
      "Epoch [22/200], Loss: 2.8142, Accuracy: 41.23%\n",
      "✅ Best model saved at epoch 22 with loss 2.8142\n",
      "Epoch [23/200], Loss: 2.7589, Accuracy: 42.12%\n",
      "✅ Best model saved at epoch 23 with loss 2.7589\n",
      "Epoch [24/200], Loss: 2.6924, Accuracy: 44.51%\n",
      "✅ Best model saved at epoch 24 with loss 2.6924\n",
      "Epoch [25/200], Loss: 2.6269, Accuracy: 46.16%\n",
      "✅ Best model saved at epoch 25 with loss 2.6269\n",
      "Epoch [26/200], Loss: 2.5604, Accuracy: 48.87%\n",
      "✅ Best model saved at epoch 26 with loss 2.5604\n",
      "Epoch [27/200], Loss: 2.4815, Accuracy: 51.62%\n",
      "✅ Best model saved at epoch 27 with loss 2.4815\n",
      "Epoch [28/200], Loss: 2.4446, Accuracy: 52.03%\n",
      "✅ Best model saved at epoch 28 with loss 2.4446\n",
      "Epoch [29/200], Loss: 2.3839, Accuracy: 54.29%\n",
      "✅ Best model saved at epoch 29 with loss 2.3839\n",
      "Epoch [30/200], Loss: 2.3094, Accuracy: 57.13%\n",
      "✅ Best model saved at epoch 30 with loss 2.3094\n",
      "Epoch [31/200], Loss: 2.2382, Accuracy: 59.31%\n",
      "✅ Best model saved at epoch 31 with loss 2.2382\n",
      "Epoch [32/200], Loss: 2.2147, Accuracy: 59.77%\n",
      "✅ Best model saved at epoch 32 with loss 2.2147\n",
      "Epoch [33/200], Loss: 2.1649, Accuracy: 61.96%\n",
      "✅ Best model saved at epoch 33 with loss 2.1649\n",
      "Epoch [34/200], Loss: 2.1630, Accuracy: 61.81%\n",
      "✅ Best model saved at epoch 34 with loss 2.1630\n",
      "Epoch [35/200], Loss: 2.0748, Accuracy: 64.30%\n",
      "✅ Best model saved at epoch 35 with loss 2.0748\n",
      "Epoch [36/200], Loss: 2.0146, Accuracy: 66.22%\n",
      "✅ Best model saved at epoch 36 with loss 2.0146\n",
      "Epoch [37/200], Loss: 1.9936, Accuracy: 66.95%\n",
      "✅ Best model saved at epoch 37 with loss 1.9936\n",
      "Epoch [38/200], Loss: 1.9721, Accuracy: 67.98%\n",
      "✅ Best model saved at epoch 38 with loss 1.9721\n",
      "Epoch [39/200], Loss: 1.8947, Accuracy: 70.81%\n",
      "✅ Best model saved at epoch 39 with loss 1.8947\n",
      "Epoch [40/200], Loss: 1.8562, Accuracy: 71.98%\n",
      "✅ Best model saved at epoch 40 with loss 1.8562\n",
      "Epoch [41/200], Loss: 1.8361, Accuracy: 72.81%\n",
      "✅ Best model saved at epoch 41 with loss 1.8361\n",
      "Epoch [42/200], Loss: 1.8130, Accuracy: 73.68%\n",
      "✅ Best model saved at epoch 42 with loss 1.8130\n",
      "Epoch [43/200], Loss: 1.7886, Accuracy: 73.91%\n",
      "✅ Best model saved at epoch 43 with loss 1.7886\n",
      "Epoch [44/200], Loss: 1.7408, Accuracy: 76.62%\n",
      "✅ Best model saved at epoch 44 with loss 1.7408\n",
      "Epoch [45/200], Loss: 1.7119, Accuracy: 76.98%\n",
      "✅ Best model saved at epoch 45 with loss 1.7119\n",
      "Epoch [46/200], Loss: 1.6804, Accuracy: 77.68%\n",
      "✅ Best model saved at epoch 46 with loss 1.6804\n",
      "Epoch [47/200], Loss: 1.6364, Accuracy: 79.22%\n",
      "✅ Best model saved at epoch 47 with loss 1.6364\n",
      "Epoch [48/200], Loss: 1.6108, Accuracy: 79.90%\n",
      "✅ Best model saved at epoch 48 with loss 1.6108\n",
      "Epoch [49/200], Loss: 1.5911, Accuracy: 80.52%\n",
      "✅ Best model saved at epoch 49 with loss 1.5911\n",
      "Epoch [50/200], Loss: 1.5655, Accuracy: 81.51%\n",
      "✅ Best model saved at epoch 50 with loss 1.5655\n",
      "Epoch [51/200], Loss: 1.5446, Accuracy: 81.77%\n",
      "✅ Best model saved at epoch 51 with loss 1.5446\n",
      "Epoch [52/200], Loss: 1.4923, Accuracy: 84.01%\n",
      "✅ Best model saved at epoch 52 with loss 1.4923\n",
      "Epoch [53/200], Loss: 1.4848, Accuracy: 84.07%\n",
      "✅ Best model saved at epoch 53 with loss 1.4848\n",
      "Epoch [54/200], Loss: 1.4786, Accuracy: 84.54%\n",
      "✅ Best model saved at epoch 54 with loss 1.4786\n",
      "Epoch [55/200], Loss: 1.4432, Accuracy: 85.58%\n",
      "✅ Best model saved at epoch 55 with loss 1.4432\n",
      "Epoch [56/200], Loss: 1.4080, Accuracy: 86.79%\n",
      "✅ Best model saved at epoch 56 with loss 1.4080\n",
      "Epoch [57/200], Loss: 1.4007, Accuracy: 86.40%\n",
      "✅ Best model saved at epoch 57 with loss 1.4007\n",
      "Epoch [58/200], Loss: 1.3898, Accuracy: 86.65%\n",
      "✅ Best model saved at epoch 58 with loss 1.3898\n",
      "Epoch [59/200], Loss: 1.3826, Accuracy: 86.93%\n",
      "✅ Best model saved at epoch 59 with loss 1.3826\n",
      "Epoch [60/200], Loss: 1.3591, Accuracy: 87.65%\n",
      "✅ Best model saved at epoch 60 with loss 1.3591\n",
      "Epoch [61/200], Loss: 1.3353, Accuracy: 88.53%\n",
      "✅ Best model saved at epoch 61 with loss 1.3353\n",
      "Epoch [62/200], Loss: 1.3164, Accuracy: 89.27%\n",
      "✅ Best model saved at epoch 62 with loss 1.3164\n",
      "Epoch [63/200], Loss: 1.3049, Accuracy: 89.87%\n",
      "✅ Best model saved at epoch 63 with loss 1.3049\n",
      "Epoch [64/200], Loss: 1.2915, Accuracy: 90.12%\n",
      "✅ Best model saved at epoch 64 with loss 1.2915\n",
      "Epoch [65/200], Loss: 1.2726, Accuracy: 90.48%\n",
      "✅ Best model saved at epoch 65 with loss 1.2726\n",
      "Epoch [66/200], Loss: 1.2596, Accuracy: 91.23%\n",
      "✅ Best model saved at epoch 66 with loss 1.2596\n",
      "Epoch [67/200], Loss: 1.2658, Accuracy: 90.69%\n",
      "Epoch [68/200], Loss: 1.2577, Accuracy: 90.62%\n",
      "✅ Best model saved at epoch 68 with loss 1.2577\n",
      "Epoch [69/200], Loss: 1.2186, Accuracy: 92.12%\n",
      "✅ Best model saved at epoch 69 with loss 1.2186\n",
      "Epoch [70/200], Loss: 1.2116, Accuracy: 92.40%\n",
      "✅ Best model saved at epoch 70 with loss 1.2116\n",
      "Epoch [71/200], Loss: 1.1906, Accuracy: 92.98%\n",
      "✅ Best model saved at epoch 71 with loss 1.1906\n",
      "Epoch [72/200], Loss: 1.2081, Accuracy: 92.34%\n",
      "Epoch [73/200], Loss: 1.1826, Accuracy: 93.23%\n",
      "✅ Best model saved at epoch 73 with loss 1.1826\n",
      "Epoch [74/200], Loss: 1.1852, Accuracy: 92.65%\n",
      "Epoch [75/200], Loss: 1.1704, Accuracy: 93.05%\n",
      "✅ Best model saved at epoch 75 with loss 1.1704\n",
      "Epoch [76/200], Loss: 1.1610, Accuracy: 93.47%\n",
      "✅ Best model saved at epoch 76 with loss 1.1610\n",
      "Epoch [77/200], Loss: 1.1426, Accuracy: 94.41%\n",
      "✅ Best model saved at epoch 77 with loss 1.1426\n",
      "Epoch [78/200], Loss: 1.1395, Accuracy: 94.12%\n",
      "✅ Best model saved at epoch 78 with loss 1.1395\n",
      "Epoch [79/200], Loss: 1.1255, Accuracy: 94.87%\n",
      "✅ Best model saved at epoch 79 with loss 1.1255\n",
      "Epoch [80/200], Loss: 1.1277, Accuracy: 94.67%\n",
      "Epoch [81/200], Loss: 1.1157, Accuracy: 95.15%\n",
      "✅ Best model saved at epoch 81 with loss 1.1157\n",
      "Epoch [82/200], Loss: 1.1023, Accuracy: 95.49%\n",
      "✅ Best model saved at epoch 82 with loss 1.1023\n",
      "Epoch [83/200], Loss: 1.0825, Accuracy: 95.80%\n",
      "✅ Best model saved at epoch 83 with loss 1.0825\n",
      "Epoch [84/200], Loss: 1.0903, Accuracy: 95.52%\n",
      "Epoch [85/200], Loss: 1.0789, Accuracy: 95.49%\n",
      "✅ Best model saved at epoch 85 with loss 1.0789\n",
      "Epoch [86/200], Loss: 1.0749, Accuracy: 95.68%\n",
      "✅ Best model saved at epoch 86 with loss 1.0749\n",
      "Epoch [87/200], Loss: 1.0567, Accuracy: 96.46%\n",
      "✅ Best model saved at epoch 87 with loss 1.0567\n",
      "Epoch [88/200], Loss: 1.0642, Accuracy: 96.02%\n",
      "Epoch [89/200], Loss: 1.0560, Accuracy: 96.17%\n",
      "✅ Best model saved at epoch 89 with loss 1.0560\n",
      "Epoch [90/200], Loss: 1.0460, Accuracy: 96.41%\n",
      "✅ Best model saved at epoch 90 with loss 1.0460\n",
      "Epoch [91/200], Loss: 1.0540, Accuracy: 96.33%\n",
      "Epoch [92/200], Loss: 1.0455, Accuracy: 96.43%\n",
      "✅ Best model saved at epoch 92 with loss 1.0455\n",
      "Epoch [93/200], Loss: 1.0395, Accuracy: 96.84%\n",
      "✅ Best model saved at epoch 93 with loss 1.0395\n",
      "Epoch [94/200], Loss: 1.0240, Accuracy: 97.16%\n",
      "✅ Best model saved at epoch 94 with loss 1.0240\n",
      "Epoch [95/200], Loss: 1.0199, Accuracy: 97.40%\n",
      "✅ Best model saved at epoch 95 with loss 1.0199\n",
      "Epoch [96/200], Loss: 1.0211, Accuracy: 96.92%\n",
      "Epoch [97/200], Loss: 1.0101, Accuracy: 97.51%\n",
      "✅ Best model saved at epoch 97 with loss 1.0101\n",
      "Epoch [98/200], Loss: 1.0182, Accuracy: 97.08%\n",
      "Epoch [99/200], Loss: 1.0131, Accuracy: 97.54%\n",
      "Epoch [100/200], Loss: 1.0151, Accuracy: 97.32%\n",
      "Early stopping triggered.\n",
      "Training completed! Best model saved at: /home/haggenmueller/asl_detection/machine_learning/models/lstm/best_lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3023638/1084232635.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set: Loss = 1.6493, Accuracy = 79.59%\n",
      "Test Set: Loss = 1.6276, Accuracy = 81.02%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "LABELS_PATH = \"/home/haggenmueller/asl_detection/machine_learning/models/lstm/label_to_index.json\"\n",
    "DATA_DIR = \"/home/haggenmueller/asl_detection/machine_learning/datasets/own_dataset/normalized_keypoints\"\n",
    "MODEL_PATH = \"/home/haggenmueller/asl_detection/machine_learning/models/lstm/best_lstm_model.pth\"\n",
    "\n",
    "# Parameters\n",
    "SEQUENCE_LENGTH = 102\n",
    "INPUT_SIZE = 225\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200  \n",
    "LR = 0.001  \n",
    "PATIENCE = 3\n",
    "\n",
    "# Load labels\n",
    "with open(LABELS_PATH, \"r\") as f:\n",
    "    label_to_index = json.load(f)\n",
    "    index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "    NUM_CLASSES = len(label_to_index)\n",
    "\n",
    "# Dataset class\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, samples, sequence_length):\n",
    "        self.samples = samples\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        keypoints = np.load(file_path)\n",
    "        \n",
    "        if keypoints.shape[0] < self.sequence_length:\n",
    "            pad = np.zeros((self.sequence_length - keypoints.shape[0], keypoints.shape[1]))\n",
    "            keypoints = np.vstack((keypoints, pad))\n",
    "        else:\n",
    "            keypoints = keypoints[:self.sequence_length]\n",
    "        \n",
    "        # Normalize keypoints\n",
    "        keypoints = (keypoints - keypoints.mean()) / keypoints.std()\n",
    "        \n",
    "        return torch.tensor(keypoints, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Load and split dataset\n",
    "label_samples = defaultdict(list)\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".npy\"):\n",
    "        filename_parts = file.split(\"_\")\n",
    "        label_name = filename_parts[1]\n",
    "        if label_name in label_to_index:\n",
    "            label_samples[label_name].append(os.path.join(DATA_DIR, file))\n",
    "\n",
    "train_samples, val_samples, test_samples = [], [], []\n",
    "\n",
    "for label, files in label_samples.items():\n",
    "    np.random.shuffle(files)\n",
    "    num_total = len(files)\n",
    "    \n",
    "    num_train = int(0.70 * num_total)\n",
    "    num_val = int(0.15 * num_total)\n",
    "    num_test = num_total - num_train - num_val\n",
    "\n",
    "    train_samples.extend([(f, label_to_index[label]) for f in files[:num_train]])\n",
    "    val_samples.extend([(f, label_to_index[label]) for f in files[num_train:num_train + num_val]])\n",
    "    test_samples.extend([(f, label_to_index[label]) for f in files[num_train + num_val:]])\n",
    "\n",
    "train_dataset = ASLDataset(train_samples, SEQUENCE_LENGTH)\n",
    "val_dataset = ASLDataset(val_samples, SEQUENCE_LENGTH)\n",
    "test_dataset = ASLDataset(test_samples, SEQUENCE_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])  # Extract last time step\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMModel(input_size=INPUT_SIZE, hidden_size=256, num_layers=2, num_classes=NUM_CLASSES)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight_ih\" in name or \"weight_hh\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0.1)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "best_loss = float(\"inf\")\n",
    "stopping_counter = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for keypoints, labels in train_loader:\n",
    "        keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(keypoints)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(torch.softmax(outputs, dim=1), 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    scheduler.step()\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        stopping_counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"✅ Best model saved at epoch {epoch+1} with loss {avg_loss:.4f}\")\n",
    "    else:\n",
    "        stopping_counter += 1\n",
    "        if stopping_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Training completed! Best model saved at:\", MODEL_PATH)\n",
    "\n",
    "# Testing the model\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "def evaluate(loader, name):\n",
    "    correct, total, loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for keypoints, labels in loader:\n",
    "            keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "            outputs = model(keypoints)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(torch.softmax(outputs, dim=1), 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{name} Set: Loss = {loss / len(loader):.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "evaluate(val_loader, \"Validation\")\n",
    "evaluate(test_loader, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
