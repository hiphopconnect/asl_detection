{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training via Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 1. Load Data -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/keypoints\"\n",
    "train_json_dir = os.path.join(data_dir, \"train/openpose_output/json\")\n",
    "val_json_dir = os.path.join(data_dir, \"val/openpose_output/json\")\n",
    "test_json_dir = os.path.join(data_dir, \"test/openpose_output/json\")\n",
    "\n",
    "csv_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "train_labels_csv = os.path.join(csv_path, \"how2sign_realigned_train.csv\")\n",
    "val_labels_csv = os.path.join(csv_path, \"how2sign_realigned_val.csv\")\n",
    "test_labels_csv = os.path.join(csv_path, \"how2sign_realigned_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ----- Training Labels -----\n",
    "train_label_df = pd.read_csv(train_labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values for training\n",
    "unique_train_sentences = sorted(set(train_label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed IDs for training\n",
    "sentence_to_id_train = {sentence: idx for idx, sentence in enumerate(unique_train_sentences)}\n",
    "\n",
    "# Create mapping: SENTENCE_NAME -> 0-indexed SENTENCE_ID for training\n",
    "label_mapping_train = {\n",
    "    name: sentence_id\n",
    "    for name, sentence_id in zip(train_label_df[\"SENTENCE_NAME\"], train_label_df[\"SENTENCE_ID\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping\n",
    "y_labels = [sentence_to_id_train[sentence_id] for sentence_id in train_label_df[\"SENTENCE_ID\"]]\n",
    "print(f\"New y_labels min: {min(y_labels)}, max: {max(y_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Validation Labels -----\n",
    "val_label_df = pd.read_csv(val_labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values for validation\n",
    "unique_val_sentences = sorted(set(val_label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed IDs for validation\n",
    "sentence_to_id_val = {sentence: idx for idx, sentence in enumerate(unique_val_sentences)}\n",
    "\n",
    "# Create mapping: SENTENCE_NAME -> 0-indexed SENTENCE_ID for validation\n",
    "label_mapping_val = {\n",
    "    name: sentence_id\n",
    "    for name, sentence_id in zip(val_label_df[\"SENTENCE_NAME\"], val_label_df[\"SENTENCE_ID\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Test Labels -----\n",
    "test_label_df = pd.read_csv(test_labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values for test\n",
    "unique_test_sentences = sorted(set(test_label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed IDs for test\n",
    "sentence_to_id_test = {sentence: idx for idx, sentence in enumerate(unique_test_sentences)}\n",
    "\n",
    "# Create mapping: SENTENCE_NAME -> 0-indexed SENTENCE_ID for test\n",
    "label_mapping_test = {\n",
    "    name: sentence_id\n",
    "    for name, sentence_id in zip(test_label_df[\"SENTENCE_NAME\"], test_label_df[\"SENTENCE_ID\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print examples to check the mappings\n",
    "print(\"Train mapping example:\", list(label_mapping_train.items())[:5])\n",
    "print(\"Val mapping example:\", list(label_mapping_val.items())[:5])\n",
    "print(\"Test mapping example:\", list(label_mapping_test.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_keypoints(json_folder, max_frames=100):\n",
    "    \"\"\"\n",
    "    Load keypoints from JSON files and return a padded sequence as a tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (max_frames, feature_dim)\n",
    "    \"\"\"\n",
    "    keypoints_sequence = []\n",
    "    required_dim = 411  # Fixed feature dimension\n",
    "\n",
    "    for frame_file in sorted(os.listdir(json_folder)):\n",
    "        frame_path = os.path.join(json_folder, frame_file)\n",
    "        with open(frame_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if \"people\" in data and len(data[\"people\"]) > 0:\n",
    "            person = data[\"people\"][0]  # First detected person\n",
    "\n",
    "            # Extract keypoints from different parts\n",
    "            pose = person.get(\"pose_keypoints_2d\", [])\n",
    "            face = person.get(\"face_keypoints_2d\", [])\n",
    "            left_hand = person.get(\"hand_left_keypoints_2d\", [])\n",
    "            right_hand = person.get(\"hand_right_keypoints_2d\", [])\n",
    "            \n",
    "            # Combine all keypoints\n",
    "            full_keypoints = pose + face + left_hand + right_hand\n",
    "            \n",
    "            # Pad or truncate to required_dim\n",
    "            if len(full_keypoints) < required_dim:\n",
    "                full_keypoints += [0.0] * (required_dim - len(full_keypoints))\n",
    "            else:\n",
    "                full_keypoints = full_keypoints[:required_dim]\n",
    "            \n",
    "            keypoints_tensor = torch.tensor(full_keypoints, dtype=torch.float32)\n",
    "            keypoints_sequence.append(keypoints_tensor)\n",
    "    \n",
    "    # If no frames were loaded, return zeros\n",
    "    if not keypoints_sequence:\n",
    "        return torch.zeros((max_frames, required_dim), dtype=torch.float32)\n",
    "    \n",
    "    # Stack tensors: (num_frames, feature_dim)\n",
    "    seq_tensor = torch.stack(keypoints_sequence)\n",
    "    \n",
    "    # Pad or truncate to max_frames\n",
    "    if seq_tensor.shape[0] < max_frames:\n",
    "        padded_sequence = torch.zeros((max_frames, required_dim), dtype=torch.float32)\n",
    "        padded_sequence[:seq_tensor.shape[0]] = seq_tensor\n",
    "    else:\n",
    "        padded_sequence = seq_tensor[:max_frames]\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Maximum number of frames per sequence (set based on dataset analysis)\n",
    "MAX_FRAMES = 200  \n",
    "\n",
    "def pad_or_truncate(sequence, max_frames=MAX_FRAMES):\n",
    "    \"\"\"Pads or truncates the sequence tensor to ensure a fixed length.\"\"\"\n",
    "    num_frames, num_features = sequence.shape\n",
    "    if num_frames < max_frames:\n",
    "        pad = torch.zeros((max_frames - num_frames, num_features),\n",
    "                          dtype=sequence.dtype, device=sequence.device)\n",
    "        sequence = torch.cat((sequence, pad), dim=0)\n",
    "    else:\n",
    "        sequence = sequence[:max_frames]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(json_dir, mapping, sentence_to_id, max_frames=MAX_FRAMES):\n",
    "    X_data, y_labels = [], []\n",
    "\n",
    "    print(f\"\\nChecking JSON directory: {json_dir}\")\n",
    "    json_folders = os.listdir(json_dir)\n",
    "    print(f\"Existing JSON folders: {json_folders[:5]}\")\n",
    "\n",
    "    for folder_name in json_folders:\n",
    "        folder_path = os.path.join(json_dir, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Skipping '{folder_name}' (not a directory)\")\n",
    "            continue\n",
    "\n",
    "        # Check if folder name exists in mapping\n",
    "        if folder_name not in mapping:\n",
    "            print(f\"Skipping: '{folder_name}' (not in mapping)\")\n",
    "            continue\n",
    "\n",
    "        # Get the sentence ID directly (String)\n",
    "        sentence_id = mapping[folder_name]\n",
    "\n",
    "        # Check if sentence ID exists in sentence_to_id\n",
    "        if sentence_id not in sentence_to_id:\n",
    "            print(f\"Skipping: Sentence ID '{sentence_id}' (not in sentence_to_id)\")\n",
    "            continue\n",
    "        \n",
    "        # print(f\"Processing: '{folder_name}' -> Sentence ID '{sentence_id}' -> Mapped ID {sentence_to_id[sentence_id]}\")\n",
    "\n",
    "        # Load keypoints and normalize\n",
    "        keypoints_sequence = load_keypoints(folder_path)\n",
    "        keypoints_sequence = pad_or_truncate(keypoints_sequence, max_frames)\n",
    "\n",
    "        X_data.append(keypoints_sequence)\n",
    "        y_labels.append(sentence_to_id[sentence_id])\n",
    "\n",
    "    if not X_data:\n",
    "        print(f\"\\n‚ö†Ô∏è  No valid data found in {json_dir} ‚ö†Ô∏è\")\n",
    "\n",
    "    X_data = torch.stack(X_data) if X_data else torch.empty(0, max_frames, 411)\n",
    "    y_labels = torch.tensor(y_labels, dtype=torch.long) if y_labels else torch.empty(0, dtype=torch.long)\n",
    "    \n",
    "    return X_data, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sentence_to_id example:\", list(sentence_to_id_train.keys())[:5])\n",
    "print(\"label_mapping values:\", list(label_mapping_train.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 2. Prepare Data for PyTorch -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for training\n",
    "X_train, y_train = process_data(train_json_dir, label_mapping_train, sentence_to_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train samples:\", X_train.shape[0])\n",
    "print(\"y_train samples:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for validation \n",
    "X_val, y_val = process_data(val_json_dir, label_mapping_val, sentence_to_id_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for testing\n",
    "X_test, y_test = process_data(test_json_dir, label_mapping_test, sentence_to_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X_batch, y_batch = zip(*batch)\n",
    "    X_batch = [torch.tensor(seq, dtype=torch.float32, device=device) for seq in X_batch]  \n",
    "    lengths = torch.tensor([len(seq) for seq in X_batch], dtype=torch.long, device=device)\n",
    "    X_batch = pad_sequence(X_batch, batch_first=True)\n",
    "\n",
    "    return X_batch, torch.tensor(y_batch, dtype=torch.long, device=device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(list(zip(X_val, y_val)), batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(list(zip(X_test, y_test)), batch_size=128, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 3. Define LSTM Model -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define an LSTM-based model for sequence classification\n",
    "class SignLanguageLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=411, hidden_dim=1024, num_layers=2, output_dim=30814, dropout=0.2):\n",
    "        super(SignLanguageLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        lstm_out, _ = self.lstm(x)  # without packing\n",
    "        last_outputs = lstm_out[:, -1, :]  # last timestep\n",
    "        return self.fc(last_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 411            # Number of keypoints per frame\n",
    "hidden_dim = 1024           # Number of hidden units in LSTM\n",
    "num_layers = 2             # Number of LSTM layers\n",
    "output_dim = len(set(label_mapping_train))  # Number of classes (0-indexed)\n",
    "dropout = 0.2              # Dropout for regularization\n",
    "\n",
    "# Optionally disable cuDNN for debugging purposes\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# Create model and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device: \", device)\n",
    "# device = torch.device(\"cpu\")\n",
    "model = SignLanguageLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "\n",
    "# Apply weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.kaiming_uniform_(param)  # Better initialization for LSTMs\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "model.apply(init_weights)  #Apply to all layers\n",
    "\n",
    "# Define loss function (CrossEntropyLoss for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (Adam works well for LSTMs)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Initialize LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single batch correctly\n",
    "X_batch, y_batch, lengths = next(iter(train_loader))\n",
    "\n",
    "# Move tensors to the correct device\n",
    "X_batch, y_batch, lengths = X_batch.to(device), y_batch.to(device), lengths.to(device)\n",
    "\n",
    "print(\"y_batch min:\", y_batch.min().item(), \"y_batch max:\", y_batch.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 4. Training -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "augmented_videos_path = f\"{base_path}/own_dataset/videos_augmented\"\n",
    "processed_folder = f\"{base_path}/own_dataset/videos_processed\"\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "# Count videos per label and determine maximum frames\n",
    "label_counts = defaultdict(int)\n",
    "label_max_frames = defaultdict(int)\n",
    "max_frames = 0\n",
    "\n",
    "def process_videos(folder, is_augmented=False):\n",
    "    global max_frames\n",
    "    for video_file in os.listdir(folder):\n",
    "        if video_file.endswith(\".mp4\"):\n",
    "            parts = video_file.rsplit(\"_\", 3) if is_augmented else video_file.rsplit(\"_\", 2)\n",
    "            if len(parts) >= 3:\n",
    "                label = parts[1]  # The label is the second element\n",
    "                video_path = os.path.join(folder, video_file)\n",
    "                \n",
    "                # Open video and count frames\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                cap.release()\n",
    "                \n",
    "                label_counts[label] += 1\n",
    "                label_max_frames[label] = max(label_max_frames[label], frame_count)\n",
    "                max_frames = max(max_frames, frame_count)\n",
    "\n",
    "# Count videos and find maximum frames in all three folders\n",
    "process_videos(raw_videos_path)\n",
    "process_videos(shortened_videos_path)\n",
    "process_videos(augmented_videos_path, is_augmented=True)\n",
    "\n",
    "print(f\"üìè Maximum number of frames: {max_frames}\")\n",
    "\n",
    "# Function to extract frames as Torch tensors\n",
    "def extract_frames(video_path, device=\"cuda\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not open video: {video_path}\")\n",
    "        return torch.zeros((1, 3, 224, 224), dtype=torch.float32, device=device)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = torch.tensor(frame, dtype=torch.float32, device=device).permute(2, 0, 1)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not frames:\n",
    "        print(f\"‚ö†Ô∏è Warning: No frames extracted for {video_path}\")\n",
    "        return torch.zeros((1, 3, 224, 224), dtype=torch.float32, device=device)\n",
    "    \n",
    "    return torch.stack(frames, dim=0)\n",
    "\n",
    "# Function to pad frames\n",
    "def pad_frames(frames, target_length, device=\"cuda\"):\n",
    "    num_frames = frames.shape[0]\n",
    "    \n",
    "    if num_frames < target_length:\n",
    "        padding = torch.zeros((target_length - num_frames, 3, 224, 224), dtype=torch.float32, device=device)\n",
    "        return torch.cat((frames, padding), dim=0)\n",
    "    else:\n",
    "        return frames[:target_length]\n",
    "\n",
    "# Process videos\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_epochs = 30  # Adjust as needed\n",
    "patience = 10   # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch, lengths in train_loader:  # Unpacking 3 values now\n",
    "        X_batch, y_batch, lengths = X_batch.to(device), y_batch.to(device), lengths.to(device)\n",
    "\n",
    "        # Forward pass with sequence lengths\n",
    "        outputs = model(X_batch, lengths)  \n",
    "\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val, lengths in val_loader:  # Unpack 3 values\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            lengths = lengths.cpu()  # Move lengths to CPU\n",
    "\n",
    "            outputs_val = model(X_val, lengths)  # Pass lengths to model\n",
    "            loss_val = criterion(outputs_val, y_val)\n",
    "            total_val_loss += loss_val.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # LR-Scheduler: Update based on Validation Loss\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"üöÄ Normalization and Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 5. Save Model & Evaluate -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Save only state_dict\n",
    "torch.save(model.state_dict(), \"sign_language_lstm_state.pth\")\n",
    "\n",
    "# Then later load it into a model instance\n",
    "model = SignLanguageLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "model.load_state_dict(torch.load(\"sign_language_lstm_state.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 6. Testing & Inference -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample_input):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Ensure input is a PyTorch tensor\n",
    "    if not isinstance(sample_input, torch.Tensor):\n",
    "        sample_input = torch.tensor(sample_input, dtype=torch.float32)\n",
    "    \n",
    "    sample_input = sample_input.unsqueeze(0).to(device)  # Add batch dimension and move to correct device\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = model(sample_input)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Example usage with test data\n",
    "# Ensure that X_test is not empty; here we take the first sample\n",
    "sample_idx = 0  # or any valid index in the test set\n",
    "sample_input = X_test[sample_idx]\n",
    "predicted_label = predict(model, sample_input)\n",
    "print(f\"Predicted Label (Test Data): {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
