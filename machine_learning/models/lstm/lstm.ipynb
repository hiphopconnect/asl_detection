{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training via Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 1. Load Data -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/geiger/asl_detection/machine_learning/datasets/how2sign/keypoints/train/openpose_output\"\n",
    "json_dir = os.path.join(data_dir, \"json\")\n",
    "video_dir = os.path.join(data_dir, \"video\")\n",
    "\n",
    "csv_path = \"/home/geiger/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "labels_csv = os.path.join(csv_path, \"how2sign_realigned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load label mapping from CSV\n",
    "label_df = pd.read_csv(labels_csv, delimiter=\"\\t\")\n",
    "label_mapping = dict(zip(label_df[\"SENTENCE_NAME\"], label_df[\"SENTENCE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(list(label_mapping.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for i, (key,value) in enumerate(label_mapping.items()):\n",
    "    print(f\"Key: {key}, Value:{value}\")\n",
    "    if(i == 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints):\n",
    "    # Normalisiere die Keypoints auf den Bereich [0, 1]\n",
    "    keypoints_min = np.min(keypoints, axis=0)\n",
    "    keypoints_max = np.max(keypoints, axis=0)\n",
    "    keypoints_normalized = (keypoints - keypoints_min) / (keypoints_max - keypoints_min + 1e-8)  # Vermeide Division durch Null\n",
    "    return keypoints_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_keypoints(json_folder):\n",
    "    keypoints_sequence = []\n",
    "    for frame_file in sorted(os.listdir(json_folder)):\n",
    "        frame_path = os.path.join(json_folder, frame_file)\n",
    "        with open(frame_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            if data[\"people\"]:  # Prüfen, ob eine Person erkannt wurde\n",
    "                person = data[\"people\"][0]\n",
    "                pose = np.array(person.get(\"pose_keypoints_2d\", []))\n",
    "                face = np.array(person.get(\"face_keypoints_2d\", []))\n",
    "                hand_left = np.array(person.get(\"hand_left_keypoints_2d\", []))\n",
    "                hand_right = np.array(person.get(\"hand_right_keypoints_2d\", []))\n",
    "\n",
    "                keypoints = np.concatenate([pose, face, hand_left, hand_right])\n",
    "            else:\n",
    "                keypoints = np.zeros(75 + 70 + 63 + 63)\n",
    "\n",
    "            keypoints_normalized = normalize_keypoints(keypoints)  # Normalisiere die Keypoints\n",
    "            keypoints_sequence.append(keypoints_normalized)\n",
    "    return np.array(keypoints_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Iterate through all JSON subfolders\n",
    "X_data, y_labels = [], []\n",
    "for sentence_name in os.listdir(json_dir):\n",
    "    sentence_folder = os.path.join(json_dir, sentence_name)\n",
    "    if os.path.isdir(sentence_folder) and sentence_name in label_mapping:\n",
    "        keypoints_sequence = load_keypoints(sentence_folder)\n",
    "        X_data.append(keypoints_sequence)  # Use all frames\n",
    "        y_labels.append(label_mapping[sentence_name])  # Use sentence ID as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(X_data)\n",
    "print (y_labels[:3])\n",
    "\n",
    "print(\"Anzahl eindeutiger Labels:\", len(set(y_labels)))\n",
    "print(\"Beispiel eines Labels:\", y_labels[0])\n",
    "print(\"Shape eines Keypoints-Samples:\", X_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays\n",
    "X_data = np.array(X_data, dtype=object)  # Variable-length sequences\n",
    "y_labels = np.array(y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 2. Prepare Data for PyTorch -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Map labels to IDs\n",
    "unique_labels = {label: idx for idx, label in enumerate(set(y_labels))}\n",
    "y_labels = np.array([unique_labels[label] for label in y_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "dataset = SignLanguageDataset(X_data, y_labels)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Ändere die DataLoader-Erstellung, um gepaddete Sequenzen zu verwenden\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: (\n",
    "        pad_sequence([item[0] for item in batch], batch_first=True),  # Padded Sequenzen\n",
    "        torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 3. Define LSTM Model -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class MultiLayerLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(MultiLayerLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Mehrere LSTM-Schichten\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer für die Klassifikation\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialisiere den versteckten Zustand und den Zellzustand\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM-Schichten\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Verwende den letzten versteckten Zustand für die Klassifikation\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "input_size = X_data[0].shape[1]\n",
    "hidden_size = 512\n",
    "num_layers = 5 # Erhöhe die Anzahl der Schichten\n",
    "num_classes = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = MultiLayerLSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Erhöhe die Lernrate\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\", factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 4. Training -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 5. Save Model & Evaluate -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lstm_sign_language.pth\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train via Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/geiger/asl_detection/machine_learning/datasets/how2sign/keypoints/train/openpose_output\"\n",
    "json_dir = os.path.join(data_dir, \"json\")\n",
    "video_dir = os.path.join(data_dir, \"video\")\n",
    "\n",
    "csv_path = \"/home/geiger/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "labels_csv = os.path.join(csv_path, \"how2sign_realigned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir_val = \"/home/geiger/asl_detection/machine_learning/datasets/how2sign/keypoints/val/openpose_output\"\n",
    "json_dir_val = os.path.join(data_dir_val, \"json\")\n",
    "video_dir_val = os.path.join(data_dir_val, \"video\")\n",
    "\n",
    "csv_path_val = \"/home/geiger/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "labels_csv_val = os.path.join(csv_path_val, \"how2sign_realigned_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load label mapping from CSV\n",
    "label_df = pd.read_csv(labels_csv, delimiter=\"\\t\")\n",
    "label_mapping = dict(zip(label_df[\"SENTENCE_NAME\"], label_df[\"SENTENCE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "label_df_val = pd.read_csv(labels_csv_val, delimiter=\"\\t\")\n",
    "label_mapping_val = dict(zip(label_df_val[\"SENTENCE_NAME\"], label_df_val[\"SENTENCE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints, target_length=100, keypoint_dim=1662):\n",
    "    keypoints = np.array(keypoints)  # Sicherstellen, dass es ein NumPy-Array ist\n",
    "\n",
    "    # Falls leeres Array, direkt zurückgeben\n",
    "    if keypoints.size == 0:\n",
    "        return np.zeros((target_length, keypoint_dim))  # Falls ganz leer, mit Nullen füllen\n",
    "\n",
    "    # Berechne Min & Max für jede Spalte (also pro Keypoint-Feature)\n",
    "    keypoints_min = np.min(keypoints, axis=0, keepdims=True)  \n",
    "    keypoints_max = np.max(keypoints, axis=0, keepdims=True)\n",
    "\n",
    "    # Vermeidung von Division durch 0: Falls min == max, dann einfach 0 setzen\n",
    "    diff = keypoints_max - keypoints_min\n",
    "    diff[diff == 0] = 1  # Falls min == max → setze diff auf 1 (damit bleibt der Wert einfach 0)\n",
    "\n",
    "    keypoints_normalized = (keypoints - keypoints_min) / diff  \n",
    "\n",
    "    seq_length = len(keypoints_normalized)\n",
    "\n",
    "    # Kürzen, falls zu lang\n",
    "    if seq_length > target_length:\n",
    "        keypoints_normalized = keypoints_normalized[:target_length]\n",
    "    \n",
    "    # Padding mit letzten Frame, falls zu kurz\n",
    "    elif seq_length < target_length:\n",
    "        padding = np.tile(keypoints_normalized[-1], (target_length - seq_length, 1))  # Wiederholt letzten Frame\n",
    "        keypoints_normalized = np.vstack((keypoints_normalized, padding))\n",
    "\n",
    "    return keypoints_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print([np.shape(x) for x in X_data_val])  # Shape jedes Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for x in X_data_val:\n",
    "    print(np.shape(x), x.dtype, type(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_keypoints(json_folder):\n",
    "    keypoints_sequence = []\n",
    "    for frame_file in sorted(os.listdir(json_folder)):\n",
    "        frame_path = os.path.join(json_folder, frame_file)\n",
    "        with open(frame_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            if data[\"people\"]:  # Prüfen, ob eine Person erkannt wurde\n",
    "                person = data[\"people\"][0]\n",
    "                pose = np.array(person.get(\"pose_keypoints_2d\", []))\n",
    "                face = np.array(person.get(\"face_keypoints_2d\", []))\n",
    "                hand_left = np.array(person.get(\"hand_left_keypoints_2d\", []))\n",
    "                hand_right = np.array(person.get(\"hand_right_keypoints_2d\", []))\n",
    "\n",
    "                keypoints = np.concatenate([pose, face, hand_left, hand_right])\n",
    "            else:\n",
    "                keypoints = np.zeros(1662)  # Verwenden Sie 1662 statt 411\n",
    "\n",
    "            keypoints_normalized = normalize_keypoints(keypoints)  # Normalisiere die Keypoints\n",
    "            keypoints_sequence.append(keypoints_normalized)\n",
    "    return np.array(keypoints_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X_data, y_labels = [], []\n",
    "\n",
    "for sentence_name in os.listdir(json_dir):\n",
    "    sentence_folder = os.path.join(json_dir, sentence_name)\n",
    "    \n",
    "    if os.path.isdir(sentence_folder) and sentence_name in label_mapping:\n",
    "        keypoints_sequence = load_keypoints(sentence_folder)  # Lade Keypoints\n",
    "        \n",
    "        X_data.append(keypoints_sequence)\n",
    "        y_labels.append(label_mapping[sentence_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X_data_val, y_labels_val = [], []\n",
    "for sentence_name_val in os.listdir(json_dir_val):\n",
    "    sentence_folder_val = os.path.join(json_dir_val, sentence_name_val)\n",
    "    \n",
    "    if os.path.isdir(sentence_folder_val) and sentence_name_val in label_mapping_val:\n",
    "        keypoints_sequence_val = load_keypoints(sentence_folder_val)  # Lade Keypoints\n",
    "                \n",
    "        X_data_val.append(keypoints_sequence_val)\n",
    "        y_labels_val.append(label_mapping_val[sentence_name_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "unique_labels = sorted(set(y_labels))  # Sortiert für konsistente IDs\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "y_labels = [label_to_id[label] for label in y_labels]\n",
    "X_train = np.array(X_data, dtype=np.float32)  # Konvertiere die Liste in ein NumPy-Array\n",
    "y_train = np.array(y_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "unique_labels_val = sorted(set(y_labels_val))  # Sortiert für konsistente IDs\n",
    "label_to_id_val = {label_val: idx for idx, label_val in enumerate(unique_labels_val)}\n",
    "\n",
    "y_labels_val = [label_to_id_val[label_val] for label_val in y_labels_val]\n",
    "X_val = np.array(X_data_val, dtype=np.float32)  # Konvertiere die Liste in ein NumPy-Array\n",
    "y_val = np.array(y_labels_val, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(\"Shape von X_train:\", X_train.shape)  # Erwartet: (Samples, 100, 1662)\n",
    "print(\"Shape von y_train:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(\"Shape von X_train:\", X_val.shape)\n",
    "print(\"Shape von y_train:\", y_val.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(100, 1662), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Für binäre Klassifikation\n",
    "\n",
    "# 4. Kompilieren des Modells\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 5. Modell trainieren\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# 6. Trainingsfortschritt ausgeben\n",
    "print(\"Training abgeschlossen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl_detection_mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
