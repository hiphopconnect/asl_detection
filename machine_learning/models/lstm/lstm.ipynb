{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training via Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 1. Load Data -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/keypoints/train/openpose_output\"\n",
    "json_dir = os.path.join(data_dir, \"json\")\n",
    "video_dir = os.path.join(data_dir, \"video\")\n",
    "\n",
    "csv_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "labels_csv = os.path.join(csv_path, \"how2sign_realigned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example mapping: [('--7E2sU6zP4_10-5-rgb_front', 0), ('--7E2sU6zP4_11-5-rgb_front', 1), ('--7E2sU6zP4_12-5-rgb_front', 2), ('--7E2sU6zP4_13-5-rgb_front', 3), ('--7E2sU6zP4_5-5-rgb_front', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Load label mapping from CSV\n",
    "label_df = pd.read_csv(labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "# Create a sorted list of unique SENTENCE_ID values\n",
    "unique_sentences = sorted(set(label_df[\"SENTENCE_ID\"]))\n",
    "# Map original SENTENCE_IDs to 0-indexed labels\n",
    "sentence_to_index = {sentence: idx for idx, sentence in enumerate(unique_sentences)}\n",
    "\n",
    "# Map SENTENCE_NAME to the corresponding 0-indexed label\n",
    "label_mapping = {name: sentence_to_index[sentence_id] for name, sentence_id in zip(label_df[\"SENTENCE_NAME\"], label_df[\"SENTENCE_ID\"])}\n",
    "\n",
    "print(\"Example mapping:\", list(label_mapping.items())[:5])\n",
    "\n",
    "# Create an array of mapped labels for training\n",
    "y_labels_mapped = np.array([label_mapping[name] for name in label_df[\"SENTENCE_NAME\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_keypoints(json_folder, max_frames=100):\n",
    "    \"\"\"\n",
    "    Load keypoints from JSON files and return a padded sequence.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (max_frames, feature_dim)\n",
    "    \"\"\"\n",
    "    keypoints_sequence = []\n",
    "\n",
    "    for frame_file in sorted(os.listdir(json_folder)):\n",
    "        frame_path = os.path.join(json_folder, frame_file)\n",
    "\n",
    "        with open(frame_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if \"people\" in data and len(data[\"people\"]) > 0:\n",
    "            person = data[\"people\"][0]  # First detected person\n",
    "\n",
    "            # Extract keypoints\n",
    "            pose = person.get(\"pose_keypoints_2d\", [])\n",
    "            face = person.get(\"face_keypoints_2d\", [])\n",
    "            left_hand = person.get(\"hand_left_keypoints_2d\", [])\n",
    "            right_hand = person.get(\"hand_right_keypoints_2d\", [])\n",
    "\n",
    "            # Ensure fixed feature size\n",
    "            full_keypoints = pose + face + left_hand + right_hand\n",
    "            full_keypoints += [0] * (411 - len(full_keypoints))  # Pad if missing keypoints\n",
    "\n",
    "            keypoints_sequence.append(np.array(full_keypoints, dtype=np.float32))\n",
    "\n",
    "    # Convert list to array\n",
    "    if len(keypoints_sequence) == 0:\n",
    "        return torch.zeros((max_frames, 411), dtype=torch.float32)\n",
    "\n",
    "    keypoints_sequence = np.array(keypoints_sequence)\n",
    "\n",
    "    # Pad or truncate\n",
    "    padded_sequence = np.zeros((max_frames, 411), dtype=np.float32)\n",
    "    seq_length = min(len(keypoints_sequence), max_frames)\n",
    "    padded_sequence[:seq_length, :] = keypoints_sequence[:seq_length, :]\n",
    "\n",
    "    return torch.tensor(padded_sequence, dtype=torch.float32)  # Shape: (max_frames, 411)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Maximum number of frames per sequence (set based on dataset analysis)\n",
    "MAX_FRAMES = 200  \n",
    "\n",
    "def pad_or_truncate(sequence, max_frames=MAX_FRAMES):\n",
    "    \"\"\"Pads or truncates the sequence to ensure a fixed length\"\"\"\n",
    "    num_frames, num_features = sequence.shape\n",
    "    if num_frames < max_frames:\n",
    "        pad = np.zeros((max_frames - num_frames, num_features))  # Pad with zeros\n",
    "        sequence = np.vstack((sequence, pad))\n",
    "    else:\n",
    "        sequence = sequence[:max_frames, :]  # Truncate sequence\n",
    "    return sequence\n",
    "\n",
    "# Create a mapping from sentences to numerical IDs\n",
    "unique_sentences = sorted(set(label_mapping.values()))  \n",
    "sentence_to_id = {sentence: idx for idx, sentence in enumerate(unique_sentences)}\n",
    "\n",
    "# Iterate through all JSON subfolders\n",
    "X_data, y_labels = [], []\n",
    "\n",
    "for sentence_name in os.listdir(json_dir):\n",
    "    sentence_folder = os.path.join(json_dir, sentence_name)\n",
    "    \n",
    "    if os.path.isdir(sentence_folder) and sentence_name in label_mapping:\n",
    "        keypoints_sequence = load_keypoints(sentence_folder)\n",
    "        \n",
    "        if keypoints_sequence.shape[0] == 0:  # Check if empty sequence\n",
    "            keypoints_sequence = torch.zeros((MAX_FRAMES, 411), dtype=torch.float32)  \n",
    "        else:\n",
    "            if keypoints_sequence.dim() == 3:  \n",
    "                keypoints_sequence = keypoints_sequence.squeeze(0).numpy()  # Remove batch dimension\n",
    "            else:\n",
    "                keypoints_sequence = keypoints_sequence.numpy()\n",
    "\n",
    "        keypoints_sequence = pad_or_truncate(keypoints_sequence)  # Ensure fixed length\n",
    "        X_data.append(keypoints_sequence)  \n",
    "\n",
    "        # Use Sentence-ID instead of full sentence\n",
    "        label = label_mapping.get(sentence_name, None)\n",
    "        if label in sentence_to_id:  \n",
    "            y_labels.append(sentence_to_id[label])  \n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_data = torch.tensor(np.array(X_data), dtype=torch.float32)\n",
    "y_labels = torch.tensor(y_labels, dtype=torch.long)  # Classification labels as IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 2. Prepare Data for PyTorch -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1033829/1335639401.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_labels = torch.tensor(y_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numerical IDs if they are still in string format\n",
    "if isinstance(y_labels.tolist()[0], str):  # Ensure correct type check\n",
    "    unique_labels = {label: idx for idx, label in enumerate(set(y_labels))}\n",
    "    y_labels = np.array([unique_labels[label] for label in y_labels])\n",
    "\n",
    "# Convert y_labels to PyTorch tensor\n",
    "y_labels = torch.tensor(y_labels, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch dataset from preprocessed tensors\n",
    "dataset = TensorDataset(X_data, y_labels)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 3. Define LSTM Model -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SignLanguageLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
    "        super(SignLanguageLSTM, self).__init__()\n",
    "        # Apply dropout only if num_layers > 1\n",
    "        dropout = dropout if num_layers > 1 else 0\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use last timestep's output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        return self.fc(last_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSignLanguageLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Loss function (CrossEntropy for classification)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/.conda/envs/asl_detection/lib/python3.13/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/asl_detection/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/asl_detection/lib/python3.13/site-packages/torch/nn/modules/rnn.py:285\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 285\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m~/.conda/envs/asl_detection/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/asl_detection/lib/python3.13/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "y_labels_mapped = np.array([0, 1, 2, 1, 0])  # Dummy example\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 411  # Number of keypoints per frame\n",
    "hidden_dim = 256  # Number of hidden units in LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_dim = len(set(y_labels_mapped))  # Number of classes (0-indexed)\n",
    "dropout = 0.3  # Dropout for regularization\n",
    "\n",
    "# Create model and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SignLanguageLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "print(\"Model moved to device successfully.\")\n",
    "\n",
    "# Test forward pass with dummy input\n",
    "dummy_input = torch.randn(2, 10, input_dim).to(device)  # Batch size 2, sequence length 10\n",
    "dummy_output = model(dummy_input)\n",
    "print(\"Dummy output shape:\", dummy_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 4. Training -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Labels - Min: 1762 Max: 30861 Type: torch.int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU Labels - Min:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_batch\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_batch\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 16\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mX_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Ensure correct label shape\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10  # Adjust as needed\n",
    "batch_size = 16  # Already set in DataLoader\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # Optional: Faster training on GPU\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        print(\"CPU Labels - Min:\", y_batch.min().item(), \"Max:\", y_batch.max().item(), \"Type:\", y_batch.dtype)\n",
    "\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Ensure correct label shape\n",
    "        y_batch = y_batch.squeeze()\n",
    "\n",
    "        # Debugging: Check tensor shapes\n",
    "        print(\"Min Label:\", y_batch.min().item(), \"Max Label:\", y_batch.max().item())\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 5. Save Model & Evaluate -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Save full model\n",
    "torch.save(model, \"sign_language_lstm_full.pth\")\n",
    "\n",
    "# Load full model\n",
    "model = torch.load(\"sign_language_lstm_full.pth\", map_location=device)\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 6. Testing & Inference -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample_input):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Ensure input is a PyTorch tensor\n",
    "    if not isinstance(sample_input, torch.Tensor):\n",
    "        sample_input = torch.tensor(sample_input, dtype=torch.float32)\n",
    "    \n",
    "    sample_input = sample_input.unsqueeze(0).to(device)  # Add batch dimension and move to correct device\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = model(sample_input)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Example usage (with a sample from dataset)\n",
    "sample_idx = min(0, len(X_data) - 1)  # Ensure valid index\n",
    "sample_input = X_data[sample_idx]  # Pick one sample from dataset\n",
    "predicted_label = predict(model, sample_input)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
