{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training via Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 1. Load Data -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_dir = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/keypoints/train/openpose_output\"\n",
    "json_dir = os.path.join(data_dir, \"json\")\n",
    "video_dir = os.path.join(data_dir, \"video\")\n",
    "\n",
    "csv_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets/how2sign/english_translation\"\n",
    "labels_csv = os.path.join(csv_path, \"how2sign_realigned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load label mapping from CSV\n",
    "label_df = pd.read_csv(labels_csv, delimiter=\"\\t\")\n",
    "\n",
    "unique_sentences = list(set(label_df[\"SENTENCE_ID\"]))\n",
    "sentence_to_index = {sentence: idx for idx, sentence in enumerate(unique_sentences)}\n",
    "\n",
    "label_mapping = {name: sentence_to_index[sentence_id] for name, sentence_id in zip(label_df[\"SENTENCE_NAME\"], label_df[\"SENTENCE_ID\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(list(label_mapping.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# for i, (key,value) in enumerate(label_mapping.items()):\n",
    "#     print(f\"Key: {key}, Value:{value}\")\n",
    "#     if(i == 5):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_keypoints(json_folder, max_frames=100):\n",
    "    \"\"\"\n",
    "    Load keypoints from JSON files and return a padded sequence.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (max_frames, feature_dim)\n",
    "    \"\"\"\n",
    "    keypoints_sequence = []\n",
    "\n",
    "    for frame_file in sorted(os.listdir(json_folder)):\n",
    "        frame_path = os.path.join(json_folder, frame_file)\n",
    "\n",
    "        with open(frame_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if \"people\" in data and len(data[\"people\"]) > 0:\n",
    "            person = data[\"people\"][0]  # First detected person\n",
    "\n",
    "            # Extract keypoints\n",
    "            pose = person.get(\"pose_keypoints_2d\", [])\n",
    "            face = person.get(\"face_keypoints_2d\", [])\n",
    "            left_hand = person.get(\"hand_left_keypoints_2d\", [])\n",
    "            right_hand = person.get(\"hand_right_keypoints_2d\", [])\n",
    "\n",
    "            # Ensure fixed feature size\n",
    "            full_keypoints = pose + face + left_hand + right_hand\n",
    "            full_keypoints += [0] * (411 - len(full_keypoints))  # Pad if missing keypoints\n",
    "\n",
    "            keypoints_sequence.append(np.array(full_keypoints, dtype=np.float32))\n",
    "\n",
    "    # Convert list to array\n",
    "    if len(keypoints_sequence) == 0:\n",
    "        return torch.zeros((max_frames, 411), dtype=torch.float32)\n",
    "\n",
    "    keypoints_sequence = np.array(keypoints_sequence)\n",
    "\n",
    "    # Pad or truncate\n",
    "    padded_sequence = np.zeros((max_frames, 411), dtype=np.float32)\n",
    "    seq_length = min(len(keypoints_sequence), max_frames)\n",
    "    padded_sequence[:seq_length, :] = keypoints_sequence[:seq_length, :]\n",
    "\n",
    "    return torch.tensor(padded_sequence, dtype=torch.float32)  # Shape: (max_frames, 411)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Maximum number of frames per sequence (set based on dataset analysis)\n",
    "MAX_FRAMES = 200  \n",
    "\n",
    "def pad_or_truncate(sequence, max_frames=MAX_FRAMES):\n",
    "    \"\"\"Pads or truncates the sequence to ensure a fixed length\"\"\"\n",
    "    num_frames, num_features = sequence.shape\n",
    "    if num_frames < max_frames:\n",
    "        pad = np.zeros((max_frames - num_frames, num_features))  # Pad with zeros\n",
    "        sequence = np.vstack((sequence, pad))\n",
    "    else:\n",
    "        sequence = sequence[:max_frames, :]  # Truncate sequence\n",
    "    return sequence\n",
    "\n",
    "# Create a mapping from sentences to numerical IDs\n",
    "unique_sentences = sorted(set(label_mapping.values()))  \n",
    "sentence_to_id = {sentence: idx for idx, sentence in enumerate(unique_sentences)}\n",
    "\n",
    "# Iterate through all JSON subfolders\n",
    "X_data, y_labels = [], []\n",
    "\n",
    "for sentence_name in os.listdir(json_dir):\n",
    "    sentence_folder = os.path.join(json_dir, sentence_name)\n",
    "    \n",
    "    if os.path.isdir(sentence_folder) and sentence_name in label_mapping:\n",
    "        keypoints_sequence = load_keypoints(sentence_folder)\n",
    "        \n",
    "        if keypoints_sequence.shape[0] == 0:  # Check if empty sequence\n",
    "            keypoints_sequence = torch.zeros((MAX_FRAMES, 411), dtype=torch.float32)  \n",
    "        else:\n",
    "            if keypoints_sequence.dim() == 3:  \n",
    "                keypoints_sequence = keypoints_sequence.squeeze(0).numpy()  # Remove batch dimension\n",
    "            else:\n",
    "                keypoints_sequence = keypoints_sequence.numpy()\n",
    "\n",
    "        keypoints_sequence = pad_or_truncate(keypoints_sequence)  # Ensure fixed length\n",
    "        X_data.append(keypoints_sequence)  \n",
    "\n",
    "        # Use Sentence-ID instead of full sentence\n",
    "        label = label_mapping.get(sentence_name, None)\n",
    "        if label in sentence_to_id:  \n",
    "            y_labels.append(sentence_to_id[label])  \n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_data = torch.tensor(np.array(X_data), dtype=torch.float32)\n",
    "y_labels = torch.tensor(y_labels, dtype=torch.long)  # Classification labels as IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape von X_data: torch.Size([31047, 200, 411])\n",
      "Anzahl eindeutiger Labels: 30814\n",
      "Beispiel eines Labels: tensor(4665)\n"
     ]
    }
   ],
   "source": [
    "# Print final shapes\n",
    "print(\"Shape von X_data:\", X_data.shape)\n",
    "print(\"Anzahl eindeutiger Labels:\", len(set(y_labels.numpy())))\n",
    "print(\"Beispiel eines Labels:\", y_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 2. Prepare Data for PyTorch -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical IDs if they are still in string format\n",
    "if isinstance(y_labels.tolist()[0], str):  # Ensure correct type check\n",
    "    unique_labels = {label: idx for idx, label in enumerate(set(y_labels))}\n",
    "    y_labels = np.array([unique_labels[label] for label in y_labels])\n",
    "\n",
    "# Convert y_labels to PyTorch tensor\n",
    "y_labels = torch.tensor(y_labels, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch dataset from preprocessed tensors\n",
    "dataset = TensorDataset(X_data, y_labels)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 3. Define LSTM Model -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define an LSTM-based model for sequence classification\n",
    "class SignLanguageLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=411, hidden_dim=256, num_layers=2, output_dim=30814, dropout=0.3):\n",
    "        super(SignLanguageLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Dropout check (PyTorch does not allow dropout for a single-layer LSTM)\n",
    "        dropout = dropout if num_layers > 1 else 0\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Take the last timestep's output\n",
    "        last_output = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(last_output)  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 411  # Number of keypoints per frame\n",
    "hidden_dim = 256  # Number of hidden units in LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_dim = len(set(y_labels.cpu().numpy()))  # Ensure y_labels is on CPU\n",
    "dropout = 0.3  # Dropout for regularization\n",
    "\n",
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SignLanguageLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "\n",
    "# Loss function (CrossEntropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam works well for LSTMs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 4. Training -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m  \u001b[38;5;66;03m# Already set in DataLoader\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_batch\u001b[49m\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be (batch_size, seq_len, feature_dim)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_batch\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be (batch_size,)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be (batch_size, num_classes)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10  # Adjust as needed\n",
    "batch_size = 16  # Already set in DataLoader\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # Optional: Faster training on GPU\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Ensure correct label shape\n",
    "        y_batch = y_batch.squeeze()\n",
    "\n",
    "        # Debugging: Check tensor shapes\n",
    "        print(f\"X_batch shape: {X_batch.shape}, y_batch shape: {y_batch.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 5. Save Model & Evaluate -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Save full model\n",
    "torch.save(model, \"sign_language_lstm_full.pth\")\n",
    "\n",
    "# Load full model\n",
    "model = torch.load(\"sign_language_lstm_full.pth\", map_location=device)\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- 6. Testing & Inference -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample_input):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Ensure input is a PyTorch tensor\n",
    "    if not isinstance(sample_input, torch.Tensor):\n",
    "        sample_input = torch.tensor(sample_input, dtype=torch.float32)\n",
    "    \n",
    "    sample_input = sample_input.unsqueeze(0).to(device)  # Add batch dimension and move to correct device\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = model(sample_input)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Example usage (with a sample from dataset)\n",
    "sample_idx = min(0, len(X_data) - 1)  # Ensure valid index\n",
    "sample_input = X_data[sample_idx]  # Pick one sample from dataset\n",
    "predicted_label = predict(model, sample_input)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
