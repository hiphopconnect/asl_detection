{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score, precision_score, recall_score\n",
    "import itertools\n",
    "\n",
    "# Konstanten\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 128  # Vom Benutzer geändert\n",
    "EPOCHS = 100  # Erhöht für mehr Trainingszeit\n",
    "LEARNING_RATE = 0.0007  # Vom Benutzer geändert\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = '/home/geiger/asl_detection/machine_learning/models/asl_now/best_model.pth'  # Pfad zum bestehenden Modell\n",
    "EARLY_STOPPING_PATIENCE = 30  # Geduld erhöht, um längeres Training zu ermöglichen\n",
    "# Problemklassen mit spezifischen Augmentationsfaktoren\n",
    "PROBLEM_CLASSES = {\n",
    "    'k': 4.0,    # Extremes Problem, wird nie richtig erkannt (F1-Score 0.000)\n",
    "    'y': 3.0,    # Hohe Precision aber sehr niedriger Recall, wird oft als 'i' erkannt\n",
    "    'p': 2.5,    # Wird oft mit 'q' verwechselt\n",
    "    's': 2.5,    # Wird oft mit 'n' verwechselt\n",
    "    'r': 2.0,    # Niedrige Precision, wird mit 'u' verwechselt\n",
    "    'q': 2.0,    # Gegenpart zu 'p', ebenfalls verstärken\n",
    "    'i': 1.8,    # Wird mit 'y' verwechselt\n",
    "    'u': 1.8     # Wird mit 'k' und 'r' verwechselt\n",
    "}\n",
    "\n",
    "# Dataset-Klasse\n",
    "class HandSignDataset(Dataset):\n",
    "    def __init__(self, keypoints, labels, augment=False):\n",
    "        self.keypoints = torch.FloatTensor(keypoints)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keypoints)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = self.keypoints[idx]\n",
    "        \n",
    "        # Einfache Datenaugmentation (nur beim Training)\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            # Leichtes zufälliges Rauschen hinzufügen (max 1.5%)\n",
    "            noise = torch.randn_like(keypoints) * 0.015\n",
    "            keypoints = keypoints + noise\n",
    "            \n",
    "        return keypoints, self.labels[idx]\n",
    "\n",
    "# Modell-Definition\n",
    "class HandSignNet(nn.Module):\n",
    "    def __init__(self, num_classes=24):\n",
    "        super(HandSignNet, self).__init__()\n",
    "        \n",
    "        # Feature Extraction Blocks\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(63, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.35),  # Leicht erhöht von 0.3\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.35),  # Leicht erhöht von 0.3\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.35)  # Leicht erhöht von 0.3\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Konfusionsmatrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Zeichnet eine Konfusionsmatrix.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Tatsächliche Klasse')\n",
    "    plt.xlabel('Vorhergesagte Klasse')\n",
    "    \n",
    "    # Speichere die Konfusionsmatrix\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analysiere die Konfusionsmatrix\n",
    "    worst_classes_idx, top_confusion_pairs = analyze_confusion_matrix(cm, classes)\n",
    "    \n",
    "    return worst_classes_idx, top_confusion_pairs\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train')\n",
    "    plt.plot(val_accs, label='Validation')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "def get_class_weights(y_train, alphabet):\n",
    "    \"\"\"\n",
    "    Berechnet Klassengewichte für unbalancierte Datasets.\n",
    "    Problemklassen werden zusätzlich stärker gewichtet.\n",
    "    \n",
    "    Args:\n",
    "        y_train: Label-Array\n",
    "        alphabet: Liste der Klassennamen\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mit Klassengewichten\n",
    "    \"\"\"\n",
    "    # Berechne Grundgewichte basierend auf Klassenverteilung\n",
    "    unique_classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=unique_classes, y=y_train)\n",
    "    \n",
    "    # Erstelle Dictionary mit Klassenindizes als Schlüssel\n",
    "    class_weights = {i: w for i, w in zip(unique_classes, weights)}\n",
    "    \n",
    "    # Dictionary zur Zuordnung von Buchstaben zu Indizes\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(alphabet)}\n",
    "    \n",
    "    # Problemspezifische zusätzliche Gewichtungsfaktoren\n",
    "    problem_weights = {\n",
    "        'k': 3.0,  # Extremes Problem\n",
    "        'y': 2.5,  # Sehr niedriger Recall\n",
    "        'p': 2.0,  # Wird oft mit 'q' verwechselt\n",
    "        's': 2.0,  # Wird oft mit 'n' verwechselt\n",
    "        'r': 1.8,  # Precision-Problem\n",
    "        'q': 1.5,  # Gegenpart zu 'p'\n",
    "        'i': 1.3,  # Wird mit 'y' verwechselt\n",
    "        'u': 1.3   # Wird mit 'k' und 'r' verwechselt\n",
    "    }\n",
    "    \n",
    "    # Verstärke Gewichte für Problemklassen basierend auf spezifischen Faktoren\n",
    "    for char, extra_factor in problem_weights.items():\n",
    "        if char in char_to_idx:\n",
    "            idx = char_to_idx[char]\n",
    "            if idx in class_weights:\n",
    "                old_weight = class_weights[idx]\n",
    "                class_weights[idx] *= extra_factor\n",
    "                print(f\"Zusätzliches Gewicht für Problemklasse '{char}': {old_weight:.2f} -> {class_weights[idx]:.2f} (Faktor {extra_factor})\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def analyze_confusion_matrix(cm, classes):\n",
    "    \"\"\"\n",
    "    Analysiert die Konfusionsmatrix, um häufige Probleme zu identifizieren\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Analyse der Konfusionsmatrix ===\")\n",
    "    \n",
    "    # Normalisierte Konfusionsmatrix für Vergleiche\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Berechne Precision, Recall und F1-Score pro Klasse\n",
    "    precision = np.zeros(len(classes))\n",
    "    recall = np.zeros(len(classes))\n",
    "    f1_score = np.zeros(len(classes))\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        # True Positives: Diagonalelement\n",
    "        tp = cm[i, i]\n",
    "        # False Positives: Summe der Spalte minus True Positives\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        # False Negatives: Summe der Zeile minus True Positives\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        \n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0\n",
    "    \n",
    "    # Sortiere nach F1-Score (aufsteigend)\n",
    "    sorted_indices = np.argsort(f1_score)\n",
    "    worst_classes_idx = sorted_indices[:5]  # Die 5 schlechtesten Klassen\n",
    "    \n",
    "    print(\"\\nDie 5 am schlechtesten erkannten Buchstaben (nach F1-Score):\")\n",
    "    print(\"Buchstabe | Precision | Recall  | F1-Score | Hauptprobleme\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for idx in worst_classes_idx:\n",
    "        class_name = classes[idx]\n",
    "        # Finde die häufigsten Verwechslungen für diese Klasse\n",
    "        confusion_row = [(classes[j], cm[idx, j]) for j in range(len(classes)) if j != idx and cm[idx, j] > 0]\n",
    "        confusion_row.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Nehme die Top 2 Verwechslungen, wenn verfügbar\n",
    "        top_confusions = confusion_row[:min(2, len(confusion_row))]\n",
    "        confusion_text = \", \".join([f\"verwechselt mit '{c}' ({n} mal)\" for c, n in top_confusions])\n",
    "        \n",
    "        # Etwas genauere Diagnose\n",
    "        diagnosis = \"\"\n",
    "        if recall[idx] < 0.3:\n",
    "            if precision[idx] > 0.7:\n",
    "                diagnosis = \"Extrem niedrige Erkennungsrate, aber wenn erkannt, dann korrekt\"\n",
    "            else:\n",
    "                diagnosis = \"Wird kaum erkannt und häufig verwechselt\"\n",
    "        elif precision[idx] < 0.3:\n",
    "            diagnosis = \"Sehr niedrige Genauigkeit, erzeugt viele Fehlalarme\"\n",
    "        \n",
    "        print(f\"{class_name.ljust(8)} | {precision[idx]:.3f}   | {recall[idx]:.3f}  | {f1_score[idx]:.3f}   | {confusion_text}\")\n",
    "        if diagnosis:\n",
    "            print(f\"DIAGNOSE: {diagnosis}\")\n",
    "    \n",
    "    # Finde häufig verwechselte Buchstabenpaare\n",
    "    print(\"\\nHäufig verwechselte Buchstabenpaare:\")\n",
    "    \n",
    "    confusion_pairs = []\n",
    "    for i in range(len(classes)):\n",
    "        row_sum = np.sum(cm[i, :])\n",
    "        for j in range(len(classes)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                # Normalisierter Wert: wie oft wurde i als j falsch klassifiziert\n",
    "                norm_value = cm[i, j] / row_sum if row_sum > 0 else 0\n",
    "                confusion_pairs.append((classes[i], classes[j], cm[i, j], norm_value))\n",
    "    \n",
    "    # Sortiere nach absoluter Anzahl der Verwechslungen\n",
    "    top_confusion_pairs = sorted(confusion_pairs, key=lambda x: x[2], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nBuchstabe | Verwechselt mit | Anzahl | % der Klasse\")\n",
    "    print(\"-\" * 50)\n",
    "    for true_class, pred_class, count, norm_value in top_confusion_pairs:\n",
    "        print(f\"{true_class.ljust(8)} | {pred_class.ljust(14)} | {count:5d} | {norm_value*100:5.1f}%\")\n",
    "    \n",
    "    return worst_classes_idx, top_confusion_pairs\n",
    "\n",
    "def augment_minority_classes(X_train, y_train, alphabet, augmentation_factor=1.5, problem_classes=None):\n",
    "    \"\"\"\n",
    "    Erhöht die Anzahl der Samples für unterrepräsentierte Klassen durch Datenaugmentation.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Feature-Array\n",
    "        y_train: Label-Array\n",
    "        alphabet: Liste der Klassennamen\n",
    "        augmentation_factor: Genereller Faktor für alle Klassen\n",
    "        problem_classes: Dict mit {Buchstabe: spez. Faktor} für Problemklassen\n",
    "    \n",
    "    Returns:\n",
    "        Erweiterte X_train und y_train Arrays\n",
    "    \"\"\"\n",
    "    # Konvertiere zu NumPy-Arrays, falls es Torch-Tensoren sind\n",
    "    if isinstance(X_train, torch.Tensor):\n",
    "        X_train = X_train.numpy()\n",
    "    if isinstance(y_train, torch.Tensor):\n",
    "        y_train = y_train.numpy()\n",
    "    \n",
    "    # Zähle die Samples pro Klasse\n",
    "    unique_labels, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Klassenverteilung vor der Augmentation:\")\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label < len(alphabet):\n",
    "            class_name = alphabet[label]\n",
    "            print(f\"Klasse {class_name}: {counts[i]} Samples\")\n",
    "    \n",
    "    # Erstelle ein Dictionary zur Zuordnung von Buchstaben zu Indizes\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(alphabet)}\n",
    "    \n",
    "    # Bestimme die Zielanzahl von Samples pro Klasse basierend auf Problemklassen\n",
    "    max_samples = max(counts)\n",
    "    target_samples = {}\n",
    "    \n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        if label >= len(alphabet):\n",
    "            continue\n",
    "            \n",
    "        class_name = alphabet[label]\n",
    "        # Standard-Augmentationsfaktor\n",
    "        factor = augmentation_factor\n",
    "        \n",
    "        # Erhöhter Faktor für Problemklassen\n",
    "        if problem_classes and class_name in problem_classes:\n",
    "            factor = problem_classes[class_name]\n",
    "            print(f\"Verwende erhöhten Augmentationsfaktor {factor} für Problemklasse '{class_name}'\")\n",
    "        \n",
    "        # Zielanzahl: Erhöhe um den Faktor, aber maximal bis zur größten Klassenanzahl\n",
    "        target_samples[label] = min(int(count * factor), max_samples)\n",
    "    \n",
    "    augmented_X = list(X_train)\n",
    "    augmented_y = list(y_train)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label >= len(alphabet):\n",
    "            continue\n",
    "            \n",
    "        # Indizes der aktuellen Klasse finden\n",
    "        indices = np.where(y_train == label)[0]\n",
    "        current_count = len(indices)\n",
    "        needed_extra = target_samples[label] - current_count\n",
    "        \n",
    "        if needed_extra <= 0:\n",
    "            continue\n",
    "        \n",
    "        class_name = alphabet[label]\n",
    "        print(f\"Augmentiere Klasse {class_name} um {needed_extra} zusätzliche Samples\")\n",
    "        \n",
    "        # Zufällige Samples auswählen (mit Zurücklegen, wenn nötig)\n",
    "        selected_indices = np.random.choice(indices, size=needed_extra, replace=True)\n",
    "        \n",
    "        # Stärkere Augmentation für Problemklassen\n",
    "        is_problem_class = problem_classes and class_name in problem_classes\n",
    "        \n",
    "        # Augmentation durchführen\n",
    "        for idx in selected_indices:\n",
    "            new_sample = X_train[idx].copy()\n",
    "            \n",
    "            # Basisrauschen für alle Klassen\n",
    "            noise_level = 0.02\n",
    "            \n",
    "            # Stärkeres Rauschen für Problemklassen\n",
    "            if is_problem_class:\n",
    "                noise_level = 0.03  # 50% mehr Rauschen\n",
    "                \n",
    "            # 1. Zufälliges Rauschen\n",
    "            noise = np.random.randn(*new_sample.shape) * noise_level\n",
    "            new_sample += noise\n",
    "            \n",
    "            # 2. Spezifische Augmentation für bestimmte Buchstaben\n",
    "            if class_name == 'k':\n",
    "                # 'k' hat ein extremes Erkennungsproblem - deutlich verstärkte Augmentation\n",
    "                # Verstärke Zeigefinger und kleinen Finger (typisch für 'k')\n",
    "                for i in range(12, 24):  # Zeigefinger\n",
    "                    if i % 3 == 0:  # x, y Koordinaten (nicht z)\n",
    "                        new_sample[i] *= random.uniform(1.10, 1.25)  # Stärkere Verstärkung\n",
    "                \n",
    "                for i in range(48, 60):  # Kleiner Finger\n",
    "                    if i % 3 == 0:  # x, y Koordinaten\n",
    "                        new_sample[i] *= random.uniform(1.10, 1.25)\n",
    "                        \n",
    "                # Extra Variationen für 'k' hinzufügen (mit höherer Wahrscheinlichkeit)\n",
    "                if random.random() > 0.3:  # Erhöhte Wahrscheinlichkeit für spezielle Augmentation\n",
    "                    # Stärkere Winkelung zwischen Fingern\n",
    "                    angle_adjust = random.uniform(0.08, 0.15)  # Stärkere Anpassung\n",
    "                    # Mittelfinger deutlicher nach innen bewegen\n",
    "                    for i in range(24, 36):  # Mittelfinger\n",
    "                        if i % 3 == 0:  # x-Koordinaten\n",
    "                            new_sample[i] -= angle_adjust\n",
    "                    # Zeigefinger stärker nach außen\n",
    "                    for i in range(12, 24):  # Zeigefinger\n",
    "                        if i % 3 == 0:  # x-Koordinaten\n",
    "                            new_sample[i] += angle_adjust * 0.7\n",
    "                    \n",
    "                    # Ringfinger leicht anpassen für bessere Unterscheidung zu 'r' und 'u'\n",
    "                    for i in range(36, 48):  # Ringfinger\n",
    "                        if i % 3 == 0:  # x-Koordinaten\n",
    "                            new_sample[i] += random.uniform(0.05, 0.12)\n",
    "            \n",
    "            elif class_name == 'y':\n",
    "                # 'y' wird oft als 'i' erkannt - verstärke die Unterschiede\n",
    "                # Verstärke den Daumen (Hauptunterschied zu 'i')\n",
    "                for i in range(0, 12):  # Daumen\n",
    "                    new_sample[i] *= random.uniform(1.08, 1.20)\n",
    "                \n",
    "                # Handgelenksrotation anpassen\n",
    "                rotation = random.uniform(0.05, 0.10)\n",
    "                for i in range(0, 63, 3):  # Alle x-Koordinaten\n",
    "                    new_sample[i] += rotation\n",
    "            \n",
    "            elif class_name == 'r':\n",
    "                # 'r' hat ein Problem mit der Precision - mehr Variation, um Überlappungen zu reduzieren\n",
    "                # Verändere die Kreuzung von Zeige- und Mittelfinger\n",
    "                for i in range(15, 30):  # Angenommene Indizes für Finger-Kreuzung\n",
    "                    new_sample[i] += random.uniform(-0.04, 0.06)  # Größere Variation\n",
    "                \n",
    "                # Zusätzliche Unterscheidung zu 'k' und 'u'\n",
    "                if random.random() > 0.5:\n",
    "                    # Verbessere die charakteristische Finger-Kreuzung\n",
    "                    for i in range(12, 36):  # Zeige- und Mittelfinger\n",
    "                        if i % 3 == 1:  # y-Koordinaten\n",
    "                            new_sample[i] += random.uniform(-0.05, 0.05)\n",
    "            \n",
    "            elif class_name == 's':\n",
    "                # 's' wird oft mit 'n' verwechselt\n",
    "                # Verstärke Daumen (Hauptunterschied zu 'n')\n",
    "                for i in range(0, 12):  # Daumen\n",
    "                    new_sample[i] *= random.uniform(1.05, 1.15)\n",
    "                \n",
    "                # Charakteristische Faustform verstärken\n",
    "                for i in range(12, 63):  # Alle Finger außer Daumen\n",
    "                    if i % 3 == 0:  # x-Koordinaten\n",
    "                        new_sample[i] *= random.uniform(0.92, 0.98)  # Leicht zusammenziehen\n",
    "            \n",
    "            elif class_name == 'p' or class_name == 'q':\n",
    "                # 'p' und 'q' können ähnlich aussehen - verstärke die Unterschiede\n",
    "                # Handgelenksrotation stärker anpassen\n",
    "                wrist_adj = random.uniform(-0.07, 0.07)\n",
    "                for i in range(0, 9):  # Handgelenkbereich\n",
    "                    new_sample[i] += wrist_adj\n",
    "                \n",
    "                # Spezifische Anpassung je nach Buchstabe\n",
    "                if class_name == 'p':\n",
    "                    # Verstärke die charakteristischen Merkmale von 'p'\n",
    "                    for i in range(36, 48):  # Ringfinger\n",
    "                        new_sample[i] *= random.uniform(1.05, 1.15)\n",
    "                else:  # q\n",
    "                    # Verstärke die charakteristischen Merkmale von 'q'\n",
    "                    for i in range(12, 24):  # Zeigefinger\n",
    "                        new_sample[i] *= random.uniform(1.05, 1.15)\n",
    "            \n",
    "            elif class_name == 'i':\n",
    "                # 'i' wird mit 'y' verwechselt - verstärke die Unterschiede\n",
    "                # Bei 'i' sollte der kleine Finger gestreckt sein\n",
    "                for i in range(48, 60):  # Kleiner Finger\n",
    "                    if i % 3 == 1:  # y-Koordinaten\n",
    "                        new_sample[i] *= random.uniform(1.05, 1.15)\n",
    "            \n",
    "            augmented_X.append(new_sample)\n",
    "            augmented_y.append(label)\n",
    "    \n",
    "    # Konvertiere zurück zu Arrays\n",
    "    augmented_X = np.array(augmented_X)\n",
    "    augmented_y = np.array(augmented_y)\n",
    "    \n",
    "    # Zeige die neue Verteilung\n",
    "    unique_labels, counts = np.unique(augmented_y, return_counts=True)\n",
    "    print(\"Klassenverteilung nach der Augmentation:\")\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label < len(alphabet):\n",
    "            class_name = alphabet[label]\n",
    "            print(f\"Klasse {class_name}: {counts[i]} Samples\")\n",
    "    \n",
    "    return augmented_X, augmented_y\n",
    "\n",
    "def plot_both_confusion_matrices(y_true, y_pred, classes, figsize=(18, 16)):\n",
    "    \"\"\"\n",
    "    Erstellt eine einzelne, gut lesbare Konfusionsmatrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Wir verwenden Seaborn für eine besser lesbare Darstellung\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Absolute Werte in der Hauptmatrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Erstelle eine schöne Heatmap mit Seaborn\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               linewidths=.5, square=True)\n",
    "    \n",
    "    plt.title('Konfusionsmatrix (absolute Werte)', fontsize=14)\n",
    "    plt.ylabel('Tatsächliche Klasse', fontsize=12)\n",
    "    plt.xlabel('Vorhergesagte Klasse', fontsize=12)\n",
    "    \n",
    "    # Diagonale Elemente hervorheben (korrekte Vorhersagen)\n",
    "    for i in range(len(classes)):\n",
    "        # Markiere die Diagonale mit einem farbigen Rand\n",
    "        plt.plot([i-.5, i+.5], [i-.5, i-.5], '-', color='green', linewidth=2)\n",
    "        plt.plot([i-.5, i+.5], [i+.5, i+.5], '-', color='green', linewidth=2)\n",
    "        plt.plot([i-.5, i-.5], [i-.5, i+.5], '-', color='green', linewidth=2)\n",
    "        plt.plot([i+.5, i+.5], [i-.5, i+.5], '-', color='green', linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Analysiere die Konfusionsmatrix\n",
    "    worst_classes_idx, top_confusion_pairs = analyze_confusion_matrix(cm, classes)\n",
    "    \n",
    "    return cm, worst_classes_idx, top_confusion_pairs\n",
    "\n",
    "def main(load_model=True):\n",
    "    # Setze Seeds für Reproduzierbarkeit\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # Alphabet-Definition\n",
    "    alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
    "    \n",
    "    # Lade Daten für Training\n",
    "    print(\"Lade Trainingsdaten...\")\n",
    "    train_data = np.load('/home/geiger/asl_detection/machine_learning/datasets/asl_now/Combined_Keypoints/asl_keypoints.npz')\n",
    "    X_train = train_data['keypoints']\n",
    "    y_train = train_data['labels']\n",
    "    \n",
    "    # Lade separate Daten für Validierung\n",
    "    print(\"Lade separate Validierungsdaten...\")\n",
    "    try:\n",
    "        val_data = np.load('/home/geiger/asl_detection/machine_learning/datasets/asl_now/Keypoints_1/asl_keypoints.npz')\n",
    "        X_val = val_data['keypoints']\n",
    "        y_val = val_data['labels']\n",
    "        print(f\"Validierungsdatensatz erfolgreich geladen: {len(X_val)} Samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der separaten Validierungsdaten: {e}\")\n",
    "        print(\"Verwende stattdessen einen Teil der Trainingsdaten für die Validierung...\")\n",
    "        # Fallback: Wenn die separate Datei nicht geladen werden kann, verwende train_test_split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train\n",
    "        )\n",
    "    \n",
    "    # Analysiere Klassenverteilung\n",
    "    train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "    val_unique, val_counts = np.unique(y_val, return_counts=True)\n",
    "    \n",
    "    print(\"\\nKlassenverteilung im Trainingsdatensatz:\")\n",
    "    for idx, count in zip(train_unique, train_counts):\n",
    "        if idx < len(alphabet):\n",
    "            print(f\"Klasse {alphabet[idx]}: {count} Samples\")\n",
    "    \n",
    "    print(\"\\nKlassenverteilung im Validierungsdatensatz:\")\n",
    "    for idx, count in zip(val_unique, val_counts):\n",
    "        if idx < len(alphabet):\n",
    "            print(f\"Klasse {alphabet[idx]}: {count} Samples\")\n",
    "    \n",
    "    # Balanciere unterrepräsentierte Klassen durch spezifische Augmentation\n",
    "    # Kommentiere die folgende Zeile aus, wenn du keine zusätzliche Klassenbalancierung willst\n",
    "    X_train, y_train = augment_minority_classes(X_train, y_train, alphabet, augmentation_factor=1.5, problem_classes=PROBLEM_CLASSES)\n",
    "    \n",
    "    # Berechne Klassengewichte für unbalanciertes Dataset\n",
    "    class_weights = get_class_weights(y_train, alphabet)\n",
    "    print(\"Verwende Klassengewichte für besseren Umgang mit Problemklassen\")\n",
    "    \n",
    "    print(f\"Trainingsdaten: {len(X_train)} Samples, Validierungsdaten: {len(X_val)} Samples\")\n",
    "    \n",
    "    # Erstelle DataLoader\n",
    "    train_dataset = HandSignDataset(X_train, y_train, augment=True)  # Augmentation aktiviert\n",
    "    val_dataset = HandSignDataset(X_val, y_val, augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialisiere Modell und lade vortrainiertes Modell, falls gewünscht\n",
    "    print(f\"Initialisiere Modell auf {DEVICE}...\")\n",
    "    model = HandSignNet(num_classes=len(alphabet)).to(DEVICE)\n",
    "    \n",
    "    # Kostenfunktion mit Klassengewichten\n",
    "    if class_weights:\n",
    "        # Konvertiere Klassengewichte zu Tensor\n",
    "        weights = torch.FloatTensor([class_weights.get(i, 1.0) for i in range(len(alphabet))])\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        print(\"Verwende gewichtete Verlustfunktion\")\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Leichte L2-Regularisierung\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    initial_val_acc = 0\n",
    "    if load_model and os.path.exists(MODEL_PATH):\n",
    "        print(f\"Lade vortrainiertes Modell von {MODEL_PATH}...\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"Vortrainiertes Modell erfolgreich geladen!\")\n",
    "        \n",
    "        # Führe eine Validierung mit dem geladenen Modell durch\n",
    "        print(\"Validiere geladenes Modell...\")\n",
    "        initial_val_loss, initial_val_acc, _, _ = validate(model, val_loader, criterion, DEVICE)\n",
    "        print(f\"Initiale Validierungs-Genauigkeit: {initial_val_acc:.2f}%\")\n",
    "    else:\n",
    "        if load_model:\n",
    "            print(f\"Kein vortrainiertes Modell gefunden unter {MODEL_PATH}. Starte mit neuem Modell.\")\n",
    "        else:\n",
    "            print(\"Training mit neuem Modell gestartet.\")\n",
    "    \n",
    "    best_val_acc = initial_val_acc\n",
    "    \n",
    "    # Early Stopping Variable\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    # Training\n",
    "    print(\"Starte Training...\")\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Learning Rate Anpassung\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Speichere Metriken\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Speichere bestes Modell\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"Neues bestes Modell gespeichert mit Accuracy: {val_acc:.2f}%\")\n",
    "            no_improve_epochs = 0  # Reset Counter\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"Keine Verbesserung seit {no_improve_epochs} Epochen\")\n",
    "        \n",
    "        # Ausgabe\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if no_improve_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early Stopping nach {epoch+1} Epochen ohne Verbesserung.\")\n",
    "            break\n",
    "    \n",
    "    # Lade bestes Modell für finale Evaluation\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(\"Lade bestes Modell für finale Evaluation...\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        _, final_acc, final_preds, final_labels = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Plotte Ergebnisse\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "        \n",
    "        # Erstelle und analysiere Konfusionsmatrix\n",
    "        cm, worst_classes_idx, top_confusion_pairs = plot_both_confusion_matrices(final_labels, final_preds, alphabet)\n",
    "        \n",
    "        # Speichere detaillierte Metriken\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(final_labels, final_preds)\n",
    "        \n",
    "        # Speichere Bericht in CSV-Datei\n",
    "        with open('classification_report.csv', 'w') as f:\n",
    "            f.write('Buchstabe,Precision,Recall,F1-Score,Support\\n')\n",
    "            for i in range(len(alphabet)):\n",
    "                f.write(f\"{alphabet[i]},{precision[i]:.3f},{recall[i]:.3f},{f1[i]:.3f},{support[i]}\\n\")\n",
    "        \n",
    "        print(\"\\nKlassifikationsbericht wurde in 'classification_report.csv' gespeichert.\")\n",
    "        print(\"Konfusionsmatrizen wurden in 'confusion_matrix.png' gespeichert.\")\n",
    "        \n",
    "        # Vorschlag für aktualisierte PROBLEM_CLASSES\n",
    "        print(\"\\nVorschlag für aktualisierte PROBLEM_CLASSES-Dictionary basierend auf den Ergebnissen:\")\n",
    "        print(\"PROBLEM_CLASSES = {\")\n",
    "        for idx in worst_classes_idx:\n",
    "            class_name = alphabet[idx]\n",
    "            factor = 4.0 if f1[idx] < 0.2 else 3.0 if f1[idx] < 0.5 else 2.5 if f1[idx] < 0.8 else 2.0 if f1[idx] < 0.9 else 1.8\n",
    "            print(f\"    '{class_name}': {factor:.1f},  # F1-Score: {f1[idx]:.3f}\")\n",
    "        print(\"}\")\n",
    "    else:\n",
    "        print(\"Kein gespeichertes Modell gefunden. Überspringe finale Evaluation.\")\n",
    "        # Verwende die Ergebnisse der letzten Epoche für die Plots\n",
    "        final_preds = val_preds\n",
    "        final_labels = val_labels\n",
    "        plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "        if len(final_labels) > 0:  # Nur wenn wir Validierungsdaten haben\n",
    "            # Erstelle und analysiere Konfusionsmatrix\n",
    "            cm, worst_classes_idx, top_confusion_pairs = plot_both_confusion_matrices(final_labels, final_preds, alphabet)\n",
    "            \n",
    "            # Speichere detaillierte Metriken\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(final_labels, final_preds)\n",
    "            \n",
    "            # Speichere Bericht in CSV-Datei\n",
    "            with open('classification_report.csv', 'w') as f:\n",
    "                f.write('Buchstabe,Precision,Recall,F1-Score,Support\\n')\n",
    "                for i in range(len(alphabet)):\n",
    "                    f.write(f\"{alphabet[i]},{precision[i]:.3f},{recall[i]:.3f},{f1[i]:.3f},{support[i]}\\n\")\n",
    "            \n",
    "            print(\"\\nKlassifikationsbericht wurde in 'classification_report.csv' gespeichert.\")\n",
    "            print(\"Konfusionsmatrizen wurden in 'confusion_matrix.png' gespeichert.\")\n",
    "    \n",
    "    print(f\"\\nBeste Validierungs-Accuracy: {best_val_acc:.2f}%\")\n",
    "    if best_val_acc > initial_val_acc:\n",
    "        print(f\"Verbesserung gegenüber initialem Modell: +{best_val_acc - initial_val_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prüfe, ob ein Kommandozeilenargument übergeben wurde\n",
    "    load_model = False  # Standard: Lade vorhandenes Modell nicht\n",
    "    \n",
    "    main(load_model) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl_detection_mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
