{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741554760.665387 1368867 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1741554760.739206 1369139 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.216.01), renderer: NVIDIA RTX A6000/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1741554760.834157 1369078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.923600 1369086 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.931467 1369090 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.934263 1369097 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.934842 1369088 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.954456 1369081 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.961464 1369095 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.963866 1369089 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741554760.974915 1369086 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "  0%|                                                                                                                              | 1/8360 [00:07<18:19:22,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 50037_secretary_44.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 2/8360 [00:20<24:58:33, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 00314_sleep_89.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 3/8360 [00:27<21:05:06,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 08672_california_33_brightness.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 4/8360 [00:37<21:16:35,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 03058_appointment_56_slow.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 5/8360 [00:47<21:58:03,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 00128_from_68_flip.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 6/8360 [00:56<22:06:24,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 22086_first_61_fast.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 7/8360 [01:07<23:02:04,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 00149_have_72_noise.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 8/8360 [01:19<24:57:07, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 40119_orange_88_slow.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                             | 9/8360 [01:31<25:14:48, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved keypoints for 38524_no_80_brightness.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MediaPipe Holistic Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Directories\n",
    "PROCESSED_DIR = \"/home/haggenmueller/asl_detection/machine_learning/datasets/own_dataset/processed_npy\"\n",
    "KEYPOINTS_DIR = \"/home/haggenmueller/asl_detection/machine_learning/datasets/own_dataset/keypoints_npy\"\n",
    "os.makedirs(KEYPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load MediaPipe Holistic\n",
    "holistic = mp_holistic.Holistic(static_image_mode=False, model_complexity=2)\n",
    "\n",
    "# Function to extract keypoints from a frame\n",
    "def extract_keypoints(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "    \n",
    "    keypoints = []\n",
    "    \n",
    "    # Face landmarks (x, y, z, visibility=1) → 468 * 4 = 1872\n",
    "    if results.face_landmarks:\n",
    "        keypoints.extend([[p.x, p.y, p.z, 1] for p in results.face_landmarks.landmark])\n",
    "    else:\n",
    "        keypoints.extend([[0, 0, 0, 1]] * 468)\n",
    "    \n",
    "    # Pose landmarks (x, y, z, visibility) → 33 * 4 = 132\n",
    "    if results.pose_landmarks:\n",
    "        keypoints.extend([[p.x, p.y, p.z, p.visibility] for p in results.pose_landmarks.landmark])\n",
    "    else:\n",
    "        keypoints.extend([[0, 0, 0, 0]] * 33)\n",
    "    \n",
    "    # Hand landmarks (x, y, z, visibility=1) → 42 * 4 = 168\n",
    "    for hand_landmarks in [results.left_hand_landmarks, results.right_hand_landmarks]:\n",
    "        if hand_landmarks:\n",
    "            keypoints.extend([[p.x, p.y, p.z, 1] for p in hand_landmarks.landmark])\n",
    "        else:\n",
    "            keypoints.extend([[0, 0, 0, 1]] * 21)\n",
    "    \n",
    "    return np.array(keypoints).flatten()\n",
    "\n",
    "# Process all .npy videos\n",
    "for npy_file in tqdm(os.listdir(PROCESSED_DIR)):\n",
    "    if not npy_file.endswith(\".npy\"):\n",
    "        continue\n",
    "    \n",
    "    video_path = os.path.join(PROCESSED_DIR, npy_file)\n",
    "    frames = np.load(video_path)\n",
    "    \n",
    "    keypoints_sequence = []\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = frame.transpose(1, 2, 0)  # Convert [C, H, W] -> [H, W, C]\n",
    "        keypoints = extract_keypoints(frame)\n",
    "        keypoints_sequence.append(keypoints)\n",
    "    \n",
    "    keypoints_sequence = np.array(keypoints_sequence)\n",
    "    \n",
    "    # Save keypoints\n",
    "    keypoints_path = os.path.join(KEYPOINTS_DIR, npy_file)\n",
    "    np.save(keypoints_path, keypoints_sequence)\n",
    "    print(f\"✅ Saved keypoints for {npy_file}\")\n",
    "\n",
    "print(\"🚀 Keypoints extraction completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asl_detection)",
   "language": "python",
   "name": "asl_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
