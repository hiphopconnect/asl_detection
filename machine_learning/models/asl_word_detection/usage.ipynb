{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# MediaPipe Setup\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_actions_from_json(json_path=\"/workspaces/asl_detection/machine_learning/datasets/asl_word_detection/actions_list.json\"):\n",
    "    \"\"\"Liest die Actions aus der JSON-Datei.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            actions = data.get(\"actions\", [])\n",
    "            if actions:\n",
    "                print(f\"Actions aus JSON geladen: {actions}\")\n",
    "                return np.array(actions)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Keine actions_list.json gefunden unter {json_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Fehler beim Lesen der JSON-Datei {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unerwarteter Fehler beim Lesen der JSON: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=126, hidden_size=64, num_layers=1, dropout=0.3):\n",
    "        super(SignLanguageModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  # Anpassbar aus JSON\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(nn.Linear(hidden_size * 2, 1), nn.Tanh())\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),  # BatchNorm wird wieder verwendet\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LSTM):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif \"bias\" in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Attention\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Weighted sum\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_cameras():\n",
    "    \"\"\"Lists all available cameras\"\"\"\n",
    "    available_cameras = []\n",
    "    for i in range(10):  # Check first 10 possible camera indices\n",
    "        cap = cv2.VideoCapture(i)\n",
    "        if cap.isOpened():\n",
    "            ret, _ = cap.read()\n",
    "            if ret:\n",
    "                available_cameras.append(i)\n",
    "            cap.release()\n",
    "    return available_cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_webcam_detection():\n",
    "    \"\"\"Starts webcam detection with the trained model.\"\"\"\n",
    "    print(\"\\nStarting webcam detection...\")\n",
    "\n",
    "    # Lade Actions direkt aus der JSON-Datei\n",
    "    actions = read_actions_from_json()\n",
    "    if actions is None:\n",
    "        print(\"Keine actions_list.json gefunden. Bitte stellen Sie sicher, dass die Datei existiert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoaded {len(actions)} actions: {actions}\")\n",
    "\n",
    "    # Lade Modellinformationen aus JSON\n",
    "    json_file = \"/workspaces/asl_detection/machine_learning/datasets/asl_word_detection/actions_list.json\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        model_info = data.get(\"model_info\", {})\n",
    "        hidden_size = model_info.get(\"hidden_size\", 64)\n",
    "        num_layers = model_info.get(\"num_layers\", 2)\n",
    "\n",
    "    print(f\"Initializing model with: hidden_size={hidden_size}, num_layers={num_layers}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = 126  # Only hands: (21 + 21) * 3 = 126\n",
    "    model = SignLanguageModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_classes=len(actions),\n",
    "        num_layers=num_layers\n",
    "    )\n",
    "\n",
    "    # Lade das Modell\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_path = \"/workspaces/asl_detection/machine_learning/models/asl_word_detection/best_model.pth\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        \n",
    "    print(f\"Modell erfolgreich geladen von {model_path}!\")\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    # Einfache Webcam-Initialisierung wie ursprünglich\n",
    "    current_camera = 0\n",
    "    cap = cv2.VideoCapture(current_camera)\n",
    "\n",
    "    sequence = []\n",
    "    prediction_history = []  # Für zeitliche Glättung\n",
    "    \n",
    "    print(\"\\nControls:\")\n",
    "    print(\"0-9: Switch camera\")\n",
    "    print(\"q: Quit\")\n",
    "\n",
    "    # Generate colors for each action\n",
    "    colors = []\n",
    "    for i in range(len(actions)):\n",
    "        # Generate evenly spaced colors in BGR\n",
    "        hue = i * 180 // len(actions)\n",
    "        color = cv2.cvtColor(np.uint8([[[hue, 255, 255]]]), cv2.COLOR_HSV2BGR)[0][0]\n",
    "        colors.append(tuple(map(int, color)))\n",
    "\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Fehler beim Lesen des Videoframes\")\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = holistic.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw landmarks (for visualization)\n",
    "            if results.pose_landmarks:  # Show but don't use\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS\n",
    "                )\n",
    "            if results.left_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS\n",
    "                )\n",
    "            if results.right_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "            # Extract ONLY hand keypoints for prediction\n",
    "            lh = np.zeros(21 * 3)\n",
    "            if results.left_hand_landmarks:\n",
    "                for i, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "                    lh[i * 3] = lm.x\n",
    "                    lh[i * 3 + 1] = lm.y\n",
    "                    lh[i * 3 + 2] = lm.z\n",
    "\n",
    "            rh = np.zeros(21 * 3)\n",
    "            if results.right_hand_landmarks:\n",
    "                for i, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "                    rh[i * 3] = lm.x\n",
    "                    rh[i * 3 + 1] = lm.y\n",
    "                    rh[i * 3 + 2] = lm.z\n",
    "\n",
    "            keypoints = np.concatenate([lh, rh])  # Only hands!\n",
    "            sequence.append(keypoints)\n",
    "            \n",
    "            # Verwende die Sequenz-Länge aus dem Training (102)\n",
    "            sequence = sequence[-102:]  # Keep all frames just like in training\n",
    "\n",
    "            if len(sequence) == 102:  # Nur vorhersagen, wenn wir genügend Frames haben\n",
    "                # Make prediction\n",
    "                with torch.no_grad():\n",
    "                    sequence_tensor = torch.FloatTensor(np.array(sequence)).unsqueeze(0)\n",
    "                    outputs = model(sequence_tensor)\n",
    "                    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "                # Kombiniere Aktionen und Wahrscheinlichkeiten und sortiere nach Wahrscheinlichkeit\n",
    "                action_probs = [(action, prob.item()) for action, prob in zip(actions, probs[0])]\n",
    "                action_probs.sort(key=lambda x: x[1], reverse=True)  # Absteigend sortieren\n",
    "                \n",
    "                # Nur die Top-k anzeigen\n",
    "                top_k = 5  # Anzahl der anzuzeigenden Top-Ergebnisse\n",
    "                top_actions = action_probs[:top_k]\n",
    "                \n",
    "                # Prüfe auf Overfitting (extrem hohe Konfidenzen)\n",
    "                overfitting_warning = False\n",
    "                if top_actions[0][1] > 0.95 and (len(top_actions) < 2 or top_actions[0][1] - top_actions[1][1] > 0.8):\n",
    "                    overfitting_warning = True\n",
    "                \n",
    "                # Überschrift\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    \"TOP 5 PREDICTIONS:\",\n",
    "                    (10, 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    (255, 255, 255),\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "                \n",
    "                # Zeige die Top-k Ergebnisse an\n",
    "                for i, (action, prob_value) in enumerate(top_actions):\n",
    "                    # Farbcodierung nach Konfidenz\n",
    "                    if prob_value > 0.3:  # Grüner Schwellenwert\n",
    "                        color = (0, 255, 0)  # Grün für hohe Konfidenz\n",
    "                    elif prob_value > 0.1:  # Gelber Schwellenwert\n",
    "                        color = (0, 255, 255)  # Gelb für mittlere Konfidenz\n",
    "                    else:\n",
    "                        color = (0, 0, 255)  # Rot für niedrige Konfidenz\n",
    "                    \n",
    "                    # Füge (Overfitting?) Markierung hinzu, wenn verdächtig\n",
    "                    warning_text = \" (Overfitting?)\" if overfitting_warning and i == 0 and prob_value > 0.95 else \"\"\n",
    "                    \n",
    "                    # Prozentbalken für visuelle Darstellung\n",
    "                    bar_width = int(prob_value * 200)  # Max 200 Pixel breit\n",
    "                    cv2.rectangle(image, (140, 40 + i * 25), (140 + bar_width, 55 + i * 25), color, -1)\n",
    "                    \n",
    "                    # Rang und Wort\n",
    "                    cv2.putText(\n",
    "                        image,\n",
    "                        f\"{i+1}. {action}:\",\n",
    "                        (10, 50 + i * 25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7,\n",
    "                        (255, 255, 255),\n",
    "                        2,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "                    \n",
    "                    # Konfidenzwert\n",
    "                    cv2.putText(\n",
    "                        image,\n",
    "                        f\"{prob_value:.2f}{warning_text}\",\n",
    "                        (145 + bar_width, 50 + i * 25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.6,\n",
    "                        (255, 255, 255),\n",
    "                        1,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "                \n",
    "                # Show current camera number\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    f\"Camera {current_camera}\",\n",
    "                    (10, image.shape[0] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (255, 255, 255),\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "            cv2.imshow(\"ASL Detection\", image)\n",
    "\n",
    "            # Process keyboard input\n",
    "            key = cv2.waitKey(10) & 0xFF\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "            # Check for number keys (0-9)\n",
    "            elif ord(\"0\") <= key <= ord(\"9\"):\n",
    "                new_camera = key - ord(\"0\")  # Convert ASCII to number\n",
    "                if new_camera != current_camera:\n",
    "                    print(f\"Switching to camera {new_camera}\")\n",
    "                    cap.release()\n",
    "                    current_camera = new_camera\n",
    "                    cap = cv2.VideoCapture(current_camera)\n",
    "                    sequence = []  # Reset sequence\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_webcam_detection() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "showcase-H-MG0u5T-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
