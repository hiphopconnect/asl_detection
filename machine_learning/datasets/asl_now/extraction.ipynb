{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordner für Buchstabe megi A nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi A\n",
      "Ordner für Buchstabe megi B nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi B\n",
      "Ordner für Buchstabe megi C nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi C\n",
      "Ordner für Buchstabe megi D nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi D\n",
      "Ordner für Buchstabe megi E nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi E\n",
      "Ordner für Buchstabe megi F nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi F\n",
      "Ordner für Buchstabe megi G nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi G\n",
      "Ordner für Buchstabe megi H nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi H\n",
      "Ordner für Buchstabe megi I nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi I\n",
      "Ordner für Buchstabe megi K nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi K\n",
      "Ordner für Buchstabe megi L nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi L\n",
      "Ordner für Buchstabe megi M nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi M\n",
      "Ordner für Buchstabe megi N nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi N\n",
      "Ordner für Buchstabe megi O nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi O\n",
      "Ordner für Buchstabe megi P nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi P\n",
      "Ordner für Buchstabe megi Q nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi Q\n",
      "Ordner für Buchstabe megi R nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi R\n",
      "Ordner für Buchstabe megi S nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi S\n",
      "Ordner für Buchstabe megi T nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi T\n",
      "Ordner für Buchstabe megi U nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi U\n",
      "Ordner für Buchstabe megi V nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi V\n",
      "Ordner für Buchstabe megi W nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi W\n",
      "Ordner für Buchstabe megi X nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi X\n",
      "Ordner für Buchstabe megi Y nicht gefunden: /workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset/megi Y\n",
      "\n",
      "Insgesamt 0 Bilder erfolgreich verarbeitet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741122204.731638   64403 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741122204.745504   64403 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['letter', 'filename'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m^9\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m^6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_detected\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m^12\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetection_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m^6.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Setze auf True für Visualisierungen \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(visualize)\u001b[0m\n\u001b[1;32m    151\u001b[0m keypoints_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_keypoints)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# CSV-Datei mit Metadaten speichern\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m metadata_df \u001b[38;5;241m=\u001b[39m \u001b[43mkeypoints_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mletter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    155\u001b[0m metadata_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Numpy-Datei mit allen Keypoints speichern\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['letter', 'filename'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MediaPipe Hands initialisieren\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Pfade konfigurieren\n",
    "DATASET_PATH = \"/workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Dataset\"\n",
    "OUTPUT_PATH = \"/workspaces/asl_detection/machine_learning/datasets/asl_now/Mix_Keypoints\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "def extract_hand_keypoints(results):\n",
    "    \"\"\"\n",
    "    Extrahiert Keypoints der signierenden Hand (rechte Hand aus Sicht des Betrachters)\n",
    "    \"\"\"\n",
    "    # Initialisiere Array für die signierende Hand (21 Keypoints mit x, y, z)\n",
    "    hand_keypoints = np.zeros(21 * 3)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Wenn mehrere Hände erkannt wurden, finde die richtige Hand\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Die Hand-Klassifikation ist aus Sicht der Kamera\n",
    "            handedness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            if handedness == \"Right\":  # Wir suchen die rechte Hand aus Sicht der Kamera\n",
    "                hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "                break\n",
    "        # Falls keine rechte Hand gefunden wurde, nimm die erste erkannte Hand\n",
    "        if np.all(hand_keypoints == 0) and results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    return hand_keypoints\n",
    "\n",
    "def process_image(image_path, hands):\n",
    "    \"\"\"\n",
    "    Verarbeitet ein einzelnes Bild und extrahiert Hand-Keypoints\n",
    "    \"\"\"\n",
    "    # Bild laden\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Fehler beim Laden des Bildes: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Farbraum für MediaPipe konvertieren (ohne weitere Transformationen)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Keypoints mit MediaPipe Hands extrahieren\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    # Wenn keine Hand erkannt wurde, versuche es mit gespiegeltem Bild\n",
    "    if not results.multi_hand_landmarks:\n",
    "        image_flipped = cv2.flip(image_rgb, 1)\n",
    "        results = hands.process(image_flipped)\n",
    "    \n",
    "    # Extrahiere Hand-Keypoints\n",
    "    keypoints = extract_hand_keypoints(results)\n",
    "    \n",
    "    return keypoints, results\n",
    "\n",
    "def save_visualization(image_path, results, letter, idx):\n",
    "    \"\"\"\n",
    "    Speichert eine Visualisierung der Handerkennung\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    # Speichere das visualisierte Bild\n",
    "    vis_dir = os.path.join(OUTPUT_PATH, \"visualizations\", letter)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    output_path = os.path.join(vis_dir, f\"{idx:04d}.png\")\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "def main(visualize=False):\n",
    "    # Alphabet mit allen Buchstaben außer 'j' und 'z'\n",
    "    alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "    \n",
    "    # Liste für die extrahierten Keypoints\n",
    "    all_keypoints = []\n",
    "    # Dictionary für Statistiken\n",
    "    stats = {letter: {'total': 0, 'not_detected': 0} for letter in alphabet}\n",
    "    \n",
    "    # MediaPipe Hands mit angepasster Erkennungsgenauigkeit initialisieren\n",
    "    with mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.2,\n",
    "            min_tracking_confidence=0.2) as hands:\n",
    "        \n",
    "        # Über alle Buchstabenordner iterieren\n",
    "        for letter in alphabet:\n",
    "            letter_dir = os.path.join(DATASET_PATH, letter)\n",
    "            if not os.path.isdir(letter_dir):\n",
    "                print(f\"Ordner für Buchstabe {letter} nicht gefunden: {letter_dir}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Verarbeite Buchstabe: {letter}\")\n",
    "            \n",
    "            # Alle PNG-Dateien im Ordner finden\n",
    "            image_files = [f for f in os.listdir(letter_dir) if f.endswith('.png')]\n",
    "            stats[letter]['total'] = len(image_files)\n",
    "            \n",
    "            # Über alle Bilder im Ordner iterieren mit Fortschrittsbalken\n",
    "            for idx, image_file in enumerate(tqdm(image_files, desc=f\"Buchstabe {letter}\")):\n",
    "                image_path = os.path.join(letter_dir, image_file)\n",
    "                \n",
    "                # Keypoints aus dem Bild extrahieren\n",
    "                result = process_image(image_path, hands)\n",
    "                if result is None:\n",
    "                    stats[letter]['not_detected'] += 1\n",
    "                    continue\n",
    "                    \n",
    "                keypoints, mp_results = result\n",
    "                \n",
    "                # Nur speichern wenn Hand erkannt wurde\n",
    "                if not np.all(keypoints == 0):\n",
    "                    # Keypoints mit Label und Dateinamen speichern\n",
    "                    keypoint_data = {\n",
    "                        'letter': letter,\n",
    "                        'filename': image_file,\n",
    "                        'keypoints': keypoints\n",
    "                    }\n",
    "                    all_keypoints.append(keypoint_data)\n",
    "                else:\n",
    "                    stats[letter]['not_detected'] += 1\n",
    "                \n",
    "                # Optional: Visualisiere jedes 50. Bild\n",
    "                if visualize and idx % 50 == 0:\n",
    "                    save_visualization(image_path, mp_results, letter, idx)\n",
    "    \n",
    "    print(f\"\\nInsgesamt {len(all_keypoints)} Bilder erfolgreich verarbeitet.\")\n",
    "    \n",
    "    # Speichern der extrahierten Keypoints\n",
    "    keypoints_df = pd.DataFrame(all_keypoints)\n",
    "    \n",
    "    # CSV-Datei mit Metadaten speichern\n",
    "    metadata_df = keypoints_df[['letter', 'filename']].copy()\n",
    "    metadata_df.to_csv(os.path.join(OUTPUT_PATH, 'metadata.csv'), index=False)\n",
    "    \n",
    "    # Numpy-Datei mit allen Keypoints speichern\n",
    "    keypoints_array = np.array([data['keypoints'] for data in all_keypoints])\n",
    "    # Anpassen der Labels auf den Index im Alphabet\n",
    "    labels = np.array([alphabet.index(data['letter']) for data in all_keypoints])\n",
    "    \n",
    "    np.savez(os.path.join(OUTPUT_PATH, 'asl_keypoints.npz'),\n",
    "             keypoints=keypoints_array,\n",
    "             labels=labels)\n",
    "    \n",
    "    print(f\"\\nKeypoints wurden gespeichert unter: {OUTPUT_PATH}\")\n",
    "    print(\"\\nStatistiken pro Buchstabe:\")\n",
    "    print(\"Buchstabe | Gesamt | Nicht erkannt | Erkennungsrate\")\n",
    "    print(\"-\" * 50)\n",
    "    for letter in alphabet:\n",
    "        total = stats[letter]['total']\n",
    "        not_detected = stats[letter]['not_detected']\n",
    "        detection_rate = ((total - not_detected) / total * 100) if total > 0 else 0\n",
    "        print(f\"{letter:^9} | {total:^6} | {not_detected:^12} | {detection_rate:^6.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(visualize=False)  # Setze auf True für Visualisierungen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Lade die Keypoints\n",
    "data = np.load('/workspaces/asl_detection/machine_learning/datasets/asl_now/Keypoints/asl_keypoints.npz')\n",
    "keypoints = data['keypoints']\n",
    "labels = data['labels']\n",
    "\n",
    "# Buchstaben-Mapping\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "\n",
    "print(\"Grundlegende Informationen:\")\n",
    "print(f\"Anzahl Samples: {len(keypoints)}\")\n",
    "print(f\"Shape der Keypoints: {keypoints.shape}\")\n",
    "print(f\"Samples pro Buchstabe:\")\n",
    "for i, letter in enumerate(alphabet):\n",
    "    count = np.sum(labels == i)\n",
    "    print(f\"{letter}: {count} Samples\")\n",
    "\n",
    "# Analyse der Keypoint-Verteilung pro Buchstabe\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Durchschnittliche Keypoint-Position pro Buchstabe\n",
    "plt.subplot(221)\n",
    "for i, letter in enumerate(alphabet):\n",
    "    letter_keypoints = keypoints[labels == i]\n",
    "    mean_pos = letter_keypoints.reshape(-1, 21, 3)[:, :, :2].mean(axis=0)\n",
    "    plt.scatter(mean_pos[:, 0], mean_pos[:, 1], label=letter, alpha=0.6)\n",
    "plt.title('Durchschnittliche Keypoint-Positionen')\n",
    "plt.xlabel('x-Koordinate')\n",
    "plt.ylabel('y-Koordinate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Verteilung der z-Werte\n",
    "plt.subplot(222)\n",
    "for i, letter in enumerate(alphabet):\n",
    "    letter_keypoints = keypoints[labels == i]\n",
    "    z_values = letter_keypoints.reshape(-1, 21, 3)[:, :, 2]\n",
    "    plt.hist(z_values.flatten(), bins=30, alpha=0.3, label=letter)\n",
    "plt.title('Verteilung der Z-Werte')\n",
    "plt.xlabel('Z-Koordinate')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Keypoint-Varianz\n",
    "plt.subplot(223)\n",
    "variances = []\n",
    "for i, letter in enumerate(alphabet):\n",
    "    letter_keypoints = keypoints[labels == i]\n",
    "    var = letter_keypoints.reshape(-1, 21, 3).var(axis=0)\n",
    "    variances.append(var.mean(axis=1))\n",
    "\n",
    "plt.boxplot([v for v in variances], labels=alphabet)\n",
    "plt.title('Keypoint-Varianz pro Buchstabe')\n",
    "plt.ylabel('Varianz')\n",
    "\n",
    "# Plot 4: Konsistenz-Matrix\n",
    "plt.subplot(224)\n",
    "consistency = np.zeros((len(alphabet), len(alphabet)))\n",
    "for i, letter1 in enumerate(alphabet):\n",
    "    kp1 = keypoints[labels == i].reshape(-1, 21, 3)\n",
    "    mean1 = kp1.mean(axis=0)\n",
    "    for j, letter2 in enumerate(alphabet):\n",
    "        kp2 = keypoints[labels == j].reshape(-1, 21, 3)\n",
    "        mean2 = kp2.mean(axis=0)\n",
    "        # Berechne Ähnlichkeit basierend auf euklidischer Distanz\n",
    "        consistency[i, j] = np.linalg.norm(mean1 - mean2)\n",
    "\n",
    "sns.heatmap(consistency, annot=True, fmt='.2f', \n",
    "            xticklabels=alphabet, yticklabels=alphabet)\n",
    "plt.title('Unterscheidbarkeits-Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('keypoint_analysis_detailed.png')\n",
    "print(\"\\nDetailierte Analyse wurde als 'keypoint_analysis_detailed.png' gespeichert.\")\n",
    "\n",
    "# Zusätzliche statistische Analysen\n",
    "print(\"\\nStatistische Analyse:\")\n",
    "print(\"-\" * 50)\n",
    "for i, letter in enumerate(alphabet):\n",
    "    letter_keypoints = keypoints[labels == i]\n",
    "    print(f\"\\nBuchstabe {letter}:\")\n",
    "    print(f\"Min/Max X: {letter_keypoints[:, ::3].min():.3f}/{letter_keypoints[:, ::3].max():.3f}\")\n",
    "    print(f\"Min/Max Y: {letter_keypoints[:, 1::3].min():.3f}/{letter_keypoints[:, 1::3].max():.3f}\")\n",
    "    print(f\"Min/Max Z: {letter_keypoints[:, 2::3].min():.3f}/{letter_keypoints[:, 2::3].max():.3f}\")\n",
    "    print(f\"Durchschnittliche Varianz: {letter_keypoints.var():.3f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
