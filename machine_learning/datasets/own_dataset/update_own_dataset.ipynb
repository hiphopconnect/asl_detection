{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "\n",
    "# Function to count frames\n",
    "def count_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    return frame_count\n",
    "\n",
    "# Store results\n",
    "frame_counts = []\n",
    "\n",
    "# Process the folder\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    filepath = os.path.join(raw_videos_path, filename)\n",
    "    if filename.endswith(\".mp4\"):  # If videos are in MP4 format\n",
    "        parts = filename.rsplit(\"_\", 2)  # Expected format: <video_id>_<label>_<frames>.mp4\n",
    "        if len(parts) == 3 and parts[2].replace(\".mp4\", \"\").isdigit():\n",
    "            video_id, label, stored_frames = parts[0], parts[1], int(parts[2].replace(\".mp4\", \"\"))\n",
    "            actual_frames = count_frames(filepath)\n",
    "            \n",
    "            # Rename file if the stored frame count is incorrect\n",
    "            new_filename = f\"{video_id}_{label}_{actual_frames}.mp4\"\n",
    "            new_filepath = os.path.join(raw_videos_path, new_filename)\n",
    "            \n",
    "            if filename != new_filename:\n",
    "                os.rename(filepath, new_filepath)\n",
    "                print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "            \n",
    "            frame_counts.append((f\"{video_id}_{label}\", actual_frames))\n",
    "\n",
    "# Sort by frame count (descending)\n",
    "frame_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Output results\n",
    "for video, frames in frame_counts:\n",
    "    print(f\"{video} : {frames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check video availabiliy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "\n",
    "# List for unreadable videos\n",
    "bad_videos = []\n",
    "\n",
    "# Process the folder\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    filepath = os.path.join(raw_videos_path, filename)\n",
    "    if filename.endswith(\".mp4\"):  # If videos are in MP4 format\n",
    "        \n",
    "        # Check if the file is too small\n",
    "        if os.path.getsize(filepath) < 1000:  # Files under 1 KB are likely corrupted\n",
    "            bad_videos.append(filename)\n",
    "            continue\n",
    "        \n",
    "        cap = cv2.VideoCapture(filepath)\n",
    "        if not cap.isOpened():\n",
    "            bad_videos.append(filename)\n",
    "        else:\n",
    "            valid = False\n",
    "            for _ in range(5):  # Try reading multiple frames\n",
    "                ret, frame = cap.read()\n",
    "                if ret and frame is not None:\n",
    "                    valid = True\n",
    "                    break\n",
    "            if not valid:\n",
    "                bad_videos.append(filename)\n",
    "        cap.release()\n",
    "\n",
    "# Output unreadable videos\n",
    "if bad_videos:\n",
    "    print(\"\\nVideos that cannot be opened:\")\n",
    "    for bad_video in bad_videos:\n",
    "        print(bad_video)\n",
    "else:\n",
    "    print(\"All videos are readable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count labels and videos per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "\n",
    "# Dictionary to store the number of videos per label\n",
    "label_counts = defaultdict(int)\n",
    "\n",
    "# Process the folder\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    if filename.endswith(\".mp4\"):  # If videos are in MP4 format\n",
    "        parts = filename.split(\"_\")  # Expected format: <video_id>_<label>_...\n",
    "        if len(parts) > 1:\n",
    "            label = parts[1]  # Assumption: The label is in the second position\n",
    "            label_counts[label] += 1\n",
    "\n",
    "# Output the number of labels and videos per label\n",
    "print(f\"Number of labels: {len(label_counts)}\")\n",
    "print(\"\\nNumber of videos per label:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get videos with more or 90 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "\n",
    "# List for videos with more than 90 frames\n",
    "videos_above_90 = []\n",
    "\n",
    "# Process the folder\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    if filename.endswith(\".mp4\"):  # If videos are in MP4 format\n",
    "        parts = filename.rsplit(\"_\", 1)  # Expected format: <video_id>_<label>_<frames>.mp4\n",
    "        if len(parts) == 2 and parts[1].replace(\".mp4\", \"\").isdigit():\n",
    "            frame_count = int(parts[1].replace(\".mp4\", \"\"))\n",
    "            if frame_count > 90:\n",
    "                videos_above_90.append(filename)\n",
    "\n",
    "# Output videos with more than 90 frames\n",
    "if videos_above_90:\n",
    "    print(f\"Number of videos with more than 90 frames: {len(videos_above_90)}\")\n",
    "    print(\"\\nVideos with more than 90 frames:\")\n",
    "    for video in videos_above_90:\n",
    "        print(video)\n",
    "else:\n",
    "    print(\"No videos with more than 90 frames found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get labels after deleting all videos with less than 90 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "\n",
    "# Dictionary to store the number of videos per label\n",
    "label_counts = defaultdict(int)\n",
    "\n",
    "# List of videos with more than 90 frames\n",
    "videos_above_90 = set()\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    if filename.endswith(\".mp4\"):  # If videos are in MP4 format\n",
    "        parts = filename.rsplit(\"_\", 1)  # Expected format: <video_id>_<label>_<frames>.mp4\n",
    "        if len(parts) == 2 and parts[1].replace(\".mp4\", \"\").isdigit():\n",
    "            frame_count = int(parts[1].replace(\".mp4\", \"\"))\n",
    "            if frame_count > 90:\n",
    "                videos_above_90.add(filename)\n",
    "\n",
    "# Process the folder, excluding videos with more than 90 frames\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    if filename.endswith(\".mp4\") and filename not in videos_above_90:\n",
    "        parts = filename.split(\"_\")  # Expected format: <video_id>_<label>_...\n",
    "        if len(parts) > 1:\n",
    "            label = parts[1]  # Assumption: The label is in the second position\n",
    "            label_counts[label] += 1\n",
    "\n",
    "# Count the number of labels with fewer than 7 videos\n",
    "labels_less_than_7 = sum(1 for count in label_counts.values() if count < 7)\n",
    "\n",
    "# Output the number of these labels\n",
    "print(labels_less_than_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shorten videos with more than 90 frames (without end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "os.makedirs(shortened_videos_path, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to extract frame count from filename\n",
    "def get_frame_count_from_filename(filename):\n",
    "    parts = filename.rsplit(\"_\", 1)  # Expected format: <video_id>_<label>_<frames>.mp4\n",
    "    if len(parts) == 2 and parts[1].replace(\".mp4\", \"\").isdigit():\n",
    "        return int(parts[1].replace(\".mp4\", \"\"))\n",
    "    return 0\n",
    "\n",
    "# Process all videos with more than 90 frames\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    if not filename.endswith(\".mp4\"):\n",
    "        continue\n",
    "    \n",
    "    frame_count = get_frame_count_from_filename(filename)\n",
    "    if frame_count <= 90:\n",
    "        print(f\"Skipped: {filename} (only {frame_count} frames)\")\n",
    "        continue\n",
    "    \n",
    "    input_video = os.path.join(raw_videos_path, filename)\n",
    "    output_video = os.path.join(shortened_videos_path, filename)\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video: {input_video}\")\n",
    "        continue\n",
    "    \n",
    "    hand_detected = False\n",
    "    start_time = None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Detect first frame with hands\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_detected = True\n",
    "            start_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Convert to seconds\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not hand_detected:\n",
    "        print(f\"‚ùå No hands detected in {filename}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Use FFmpeg to trim video from first detected hand frame\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", input_video,\n",
    "        \"-ss\", str(start_time),  # Start from detected hand frame\n",
    "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"18\",\n",
    "        \"-pix_fmt\", \"yuv420p\",\n",
    "        output_video\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Trimming {filename} from {start_time} seconds...\")\n",
    "    process = subprocess.run(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(f\"‚úÖ Trimmed and saved: {output_video}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error processing {output_video}\")\n",
    "        print(process.stderr.decode())\n",
    "\n",
    "print(\"Done! Videos have been trimmed based on first hand detection frame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shorten videos with more than 90 frames (with end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "os.makedirs(shortened_videos_path, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to extract frame count from filename\n",
    "def get_frame_count_from_filename(filename):\n",
    "    parts = filename.rsplit(\"_\", 1)  # Expected format: <video_id>_<label>_<frames>.mp4\n",
    "    if len(parts) == 2 and parts[1].replace(\".mp4\", \"\").isdigit():\n",
    "        return int(parts[1].replace(\".mp4\", \"\"))\n",
    "    return 0\n",
    "\n",
    "# Process all videos with more than 90 frames\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    if not filename.endswith(\".mp4\"):\n",
    "        continue\n",
    "    \n",
    "    frame_count = get_frame_count_from_filename(filename)\n",
    "    if frame_count <= 90:\n",
    "        print(f\"Skipped: {filename} (only {frame_count} frames)\")\n",
    "        continue\n",
    "    \n",
    "    input_video = os.path.join(raw_videos_path, filename)\n",
    "    output_video = os.path.join(shortened_videos_path, filename)\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video: {input_video}\")\n",
    "        continue\n",
    "    \n",
    "    hand_detected = False\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Detect first and last frame with hands\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Convert to seconds\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            if not hand_detected:\n",
    "                start_time = current_time  # First frame where hands appear\n",
    "                hand_detected = True\n",
    "            end_time = current_time  # Last frame where hands were detected\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not hand_detected:\n",
    "        print(f\"‚ùå No hands detected in {filename}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    if end_time is None:\n",
    "        end_time = start_time + (90 / fps)  # Default to 90 frames if end_time isn't found\n",
    "    \n",
    "    # Use FFmpeg to trim video from first detected hand frame to last detected hand frame\n",
    "    ffmpeg_trim_command = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", input_video,\n",
    "        \"-ss\", str(start_time),  # Start from detected hand frame\n",
    "        \"-to\", str(end_time),  # Stop when hands disappear\n",
    "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"18\",\n",
    "        \"-pix_fmt\", \"yuv420p\",\n",
    "        output_video\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Trimming {filename} from {start_time} to {end_time} seconds...\")\n",
    "    process = subprocess.run(ffmpeg_trim_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(f\"‚úÖ Trimmed and saved: {output_video} (from {start_time} to {end_time})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error processing {output_video}\")\n",
    "        print(process.stderr.decode())\n",
    "\n",
    "print(\"Done! Videos have been trimmed from first to last detected hand frame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update frames in naming for every shorten video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "\n",
    "# Function to count frames\n",
    "def count_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    return frame_count\n",
    "\n",
    "# Store results\n",
    "frame_counts = []\n",
    "\n",
    "# Process the folder\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    filepath = os.path.join(raw_videos_path, filename)\n",
    "    if filename.endswith(\".mp4\"):  # If videos are in MP4 format\n",
    "        parts = filename.rsplit(\"_\", 2)  # Expected format: <video_id>_<label>_<frames>.mp4\n",
    "        if len(parts) == 3 and parts[2].replace(\".mp4\", \"\").isdigit():\n",
    "            video_id, label, stored_frames = parts[0], parts[1], int(parts[2].replace(\".mp4\", \"\"))\n",
    "            actual_frames = count_frames(filepath)\n",
    "            \n",
    "            # Rename file if the stored frame count is incorrect\n",
    "            new_filename = f\"{video_id}_{label}_{actual_frames}.mp4\"\n",
    "            new_filepath = os.path.join(raw_videos_path, new_filename)\n",
    "            \n",
    "            if filename != new_filename:\n",
    "                os.rename(filepath, new_filepath)\n",
    "                print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "            \n",
    "            frame_counts.append((f\"{video_id}_{label}\", actual_frames))\n",
    "\n",
    "# Sort by frame count (descending)\n",
    "frame_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Output results\n",
    "for video, frames in frame_counts:\n",
    "    print(f\"{video} : {frames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all duplicate videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "\n",
    "# Get video IDs from shortened_videos\n",
    "shortened_video_ids = {file.split(\"_\")[0] for file in os.listdir(shortened_videos_path) if file.endswith(\".mp4\")}\n",
    "\n",
    "# Count duplicates in raw_videos\n",
    "duplicate_count = sum(1 for file in os.listdir(raw_videos_path) if file.endswith(\".mp4\") and file.split(\"_\")[0] in shortened_video_ids)\n",
    "\n",
    "# Print result\n",
    "print(f\"üìä Number of duplicate videos in raw_videos: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates from raw_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "unused_videos_path = f\"{base_path}/wlasl/raw_videos_unused\"\n",
    "os.makedirs(unused_videos_path, exist_ok=True)\n",
    "\n",
    "# Get video IDs from shortened_videos\n",
    "shortened_video_ids = {file.split(\"_\")[0] for file in os.listdir(shortened_videos_path) if file.endswith(\".mp4\")}\n",
    "\n",
    "# Move duplicate videos to raw_videos_unused\n",
    "duplicate_count = 0\n",
    "for file in os.listdir(raw_videos_path):\n",
    "    if file.endswith(\".mp4\") and file.split(\"_\")[0] in shortened_video_ids:\n",
    "        shutil.move(os.path.join(raw_videos_path, file), os.path.join(unused_videos_path, file))\n",
    "        duplicate_count += 1\n",
    "\n",
    "# Print result\n",
    "print(f\"üìä Moved {duplicate_count} duplicate videos from raw_videos to raw_videos_unused.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> start here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get video count per label and max frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "augmented_videos_path = f\"{base_path}/own_dataset/videos_augmented\"\n",
    "\n",
    "# Count videos per label and determine maximum frames\n",
    "label_counts = defaultdict(int)\n",
    "label_max_frames = defaultdict(int)\n",
    "\n",
    "def process_videos(folder, is_augmented=False):\n",
    "    for video_file in os.listdir(folder):\n",
    "        if video_file.endswith(\".mp4\"):\n",
    "            parts = video_file.rsplit(\"_\", 3) if is_augmented else video_file.rsplit(\"_\", 2)\n",
    "            if len(parts) >= 3:\n",
    "                label = parts[1]  # The label is the second element\n",
    "                video_path = os.path.join(folder, video_file)\n",
    "                \n",
    "                # Open video and count frames\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                cap.release()\n",
    "                \n",
    "                label_counts[label] += 1\n",
    "                label_max_frames[label] = max(label_max_frames[label], frame_count)\n",
    "\n",
    "# Count videos and find maximum frames in all three folders\n",
    "process_videos(raw_videos_path)\n",
    "process_videos(shortened_videos_path)\n",
    "process_videos(augmented_videos_path, is_augmented=True)\n",
    "\n",
    "# Display results\n",
    "for label in sorted(label_counts.keys()):\n",
    "    print(f\"Label: {label}, Videos: {label_counts[label]}, Maximum Frames: {label_max_frames[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename augmented vdieos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "augmented_videos_path = f\"{base_path}/own_dataset/videos_augmented\"\n",
    "\n",
    "# Count videos per label and determine maximum frames\n",
    "label_counts = defaultdict(int)\n",
    "label_max_frames = defaultdict(int)\n",
    "\n",
    "def process_videos(folder, is_augmented=False):\n",
    "    for video_file in os.listdir(folder):\n",
    "        if video_file.endswith(\".mp4\"):\n",
    "            parts = video_file.rsplit(\"_\", 3) if is_augmented else video_file.rsplit(\"_\", 2)\n",
    "            if len(parts) >= 3:\n",
    "                label = parts[1]  # The label is the second element\n",
    "                video_path = os.path.join(folder, video_file)\n",
    "                \n",
    "                # Open video and count frames\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                cap.release()\n",
    "                \n",
    "                label_counts[label] += 1\n",
    "                label_max_frames[label] = max(label_max_frames[label], frame_count)\n",
    "\n",
    "# Count videos and find maximum frames in all three folders\n",
    "process_videos(raw_videos_path)\n",
    "process_videos(shortened_videos_path)\n",
    "process_videos(augmented_videos_path, is_augmented=True)\n",
    "\n",
    "# Verify and update filenames in augmented videos folder\n",
    "for video_file in os.listdir(augmented_videos_path):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        parts = video_file.rsplit(\"_\", 3)  # <video_id>_<label>_<frames>_<change>.mp4\n",
    "        if len(parts) == 4:\n",
    "            video_id, label, old_frames, change = parts\n",
    "            old_frames = old_frames.replace(\".mp4\", \"\")\n",
    "            \n",
    "            video_path = os.path.join(augmented_videos_path, video_file)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            \n",
    "            # Rename file if frame count is incorrect\n",
    "            if str(frame_count) != old_frames:\n",
    "                new_filename = f\"{video_id}_{label}_{frame_count}_{change}.mp4\"\n",
    "                new_filepath = os.path.join(augmented_videos_path, new_filename)\n",
    "                os.rename(video_path, new_filepath)\n",
    "                print(f\"Renamed: {video_file} -> {new_filename}\")\n",
    "\n",
    "# Display results\n",
    "for label in sorted(label_counts.keys()):\n",
    "    print(f\"Label: {label}, Videos: {label_counts[label]}, Maximum Frames: {label_max_frames[label]}\")\n",
    "\n",
    "print(\"‚úÖ All augmented videos have been verified and renamed if necessary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories\n",
    "base_path = \"/home/haggenmueller/asl_detection/machine_learning/datasets\"\n",
    "raw_videos_path = f\"{base_path}/wlasl/raw_videos\"\n",
    "shortened_videos_path = f\"{base_path}/own_dataset/shortened_videos\"\n",
    "augmented_videos_path = f\"{base_path}/own_dataset/videos_augmented\"\n",
    "processed_folder = f\"{base_path}/own_dataset/videos_processed\"\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "# Count videos per label and determine maximum frames\n",
    "label_counts = defaultdict(int)\n",
    "label_max_frames = defaultdict(int)\n",
    "max_frames = 0\n",
    "\n",
    "def process_videos(folder, is_augmented=False):\n",
    "    global max_frames\n",
    "    for video_file in os.listdir(folder):\n",
    "        if video_file.endswith(\".mp4\"):\n",
    "            parts = video_file.rsplit(\"_\", 3) if is_augmented else video_file.rsplit(\"_\", 2)\n",
    "            if len(parts) >= 3:\n",
    "                label = parts[1]  # The label is the second element\n",
    "                video_path = os.path.join(folder, video_file)\n",
    "                \n",
    "                # Open video and count frames\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                cap.release()\n",
    "                \n",
    "                label_counts[label] += 1\n",
    "                label_max_frames[label] = max(label_max_frames[label], frame_count)\n",
    "                max_frames = max(max_frames, frame_count)\n",
    "\n",
    "# Count videos and find maximum frames in all three folders\n",
    "process_videos(raw_videos_path)\n",
    "process_videos(shortened_videos_path)\n",
    "process_videos(augmented_videos_path, is_augmented=True)\n",
    "\n",
    "print(f\"üìè Maximum number of frames: {max_frames}\")\n",
    "\n",
    "# Function to extract frames as Torch tensors\n",
    "def extract_frames(video_path, device=\"cuda\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not open video: {video_path}\")\n",
    "        return torch.zeros((1, 3, 224, 224), dtype=torch.float32, device=device)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = torch.tensor(frame, dtype=torch.float32, device=device).permute(2, 0, 1)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not frames:\n",
    "        print(f\"‚ö†Ô∏è Warning: No frames extracted for {video_path}\")\n",
    "        return torch.zeros((1, 3, 224, 224), dtype=torch.float32, device=device)\n",
    "    \n",
    "    return torch.stack(frames, dim=0)\n",
    "\n",
    "# Function to pad frames\n",
    "def pad_frames(frames, target_length, device=\"cuda\"):\n",
    "    num_frames = frames.shape[0]\n",
    "    \n",
    "    if num_frames < target_length:\n",
    "        padding = torch.zeros((target_length - num_frames, 3, 224, 224), dtype=torch.float32, device=device)\n",
    "        return torch.cat((frames, padding), dim=0)\n",
    "    else:\n",
    "        return frames[:target_length]\n",
    "\n",
    "# Process videos\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for folder in [shortened_videos_path, raw_videos_path, augmented_videos_path]:\n",
    "    for video_file in os.listdir(folder):\n",
    "        if not video_file.endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(folder, video_file)\n",
    "        \n",
    "        try:\n",
    "            # Extract frames\n",
    "            frames = extract_frames(video_path, device=device)\n",
    "\n",
    "            # Pad/Trim to `max_frames`\n",
    "            padded_frames = pad_frames(frames, max_frames, device=device)\n",
    "\n",
    "            # Save as `.npy` file\n",
    "            npy_path = os.path.join(processed_folder, video_file.replace(\".mp4\", \".npy\"))\n",
    "            np.save(npy_path, padded_frames.cpu().numpy())\n",
    "\n",
    "            print(f\"‚úÖ {video_file} processed and saved as {npy_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {video_file}: {e}\")\n",
    "\n",
    "print(\"üöÄ Normalization completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
